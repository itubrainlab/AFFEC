{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import random\n",
    "def extract_segment(dataframe, stim_file, flag):\n",
    "    # Filter to get rows with matching 'stim_file' and 'flag'\n",
    "    filtered_df = dataframe[(dataframe['stim_file'] == stim_file)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        return None  # Return None if no matching rows found\n",
    "    \n",
    "    # Assuming the first matching row is the one to start from\n",
    "    initial_row = filtered_df.iloc[0]\n",
    "    initial_timestamp = initial_row['onset']\n",
    "    duration=0\n",
    "    for flag in filtered_df['flag'].unique():\n",
    "        flag_filtered_df=filtered_df[filtered_df['flag']==flag]\n",
    "        initial_row = flag_filtered_df.iloc[0]\n",
    "        duration = duration+initial_row['duration']\n",
    "        if flag==\"trial\":\n",
    "            break\n",
    "    \n",
    "    # Calculate the end timestamp\n",
    "    end_timestamp = initial_timestamp + duration\n",
    "    \n",
    "    # Select rows from the initial timestamp up to and including where Timestamp <= end_timestamp\n",
    "    result_df = dataframe[(dataframe['onset'] >= initial_timestamp) & (dataframe['onset'] <= end_timestamp)]\n",
    "    \n",
    "    return result_df\n",
    "participants_df=pd.read_csv(\"participants.tsv\",sep=\"\\t\")\n",
    "participants_df['participant_id'].unique()\n",
    "au_columns_subset = ['onset','confidence',\n",
    " 'success', 'AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', \n",
    "                     'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r', \n",
    "                     'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r']\n",
    "# Load headers from JSON\n",
    "# user='sub-agk'\n",
    "# run=0 sub-pkvd/beh/sub-pkvd_task-fer_run-3_recording-videostream_physio.tsv.gz\n",
    "missing=[]\n",
    "metrics_all=[]\n",
    "stim_file=\"\"\n",
    "for user in participants_df['participant_id'].unique():\n",
    "    \n",
    "    user_df=participants_df[participants_df['participant_id']==user]\n",
    "    openness = user_df['O'].values[0]\n",
    "    conscientiousness = user_df['C'].values[0]\n",
    "    extraversion = user_df['E'].values[0]\n",
    "    agreeableness = user_df['A'].values[0]\n",
    "    neuroticism = user_df['N'].values[0]\n",
    "    for run in range(4):\n",
    "        print(user,run)\n",
    "        json_path=user+\"/beh/\"+user+\"_task-fer_run-\"+str(run)+\"_recording-videostream_physio.json\"\n",
    "        if os.path.exists(json_path):\n",
    "            au_path=user+\"/beh/\"+user+\"_task-fer_run-\"+str(run)+\"_recording-videostream_physio.tsv.gz\"\n",
    "            events_path=user+\"/\"+user+\"_task-fer_run-\"+str(run)+\"_events.tsv\"\n",
    "            labels_path=user+\"/beh/\"+user+\"_task-fer_run-\"+str(run)+\"_beh.tsv\"\n",
    "            with open(json_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            headers = json_data['Columns']\n",
    "            # sampling_rate=len(au_data['onset'])/(au_data['onset'].iloc[-1]-au_data['onset'].iloc[0])\n",
    "            # Read the gzipped TSV file using the loaded headers\n",
    "            au_data= pd.read_csv(au_path,sep='\\t', compression='gzip', names=headers)\n",
    "            # print(au_data.head())\n",
    "            # au_data['onset'] = pd.to_numeric(au_data['onset'], errors='coerce')\n",
    "            for tag in au_columns_subset:\n",
    "                au_data[tag] = pd.to_numeric(au_data[tag], errors='coerce')\n",
    "            # gsr_data['Timestamp'] = pd.to_numeric(gsr_data['Timestamp'], errors='coerce')\n",
    "            au_data['onset']=au_data['onset']-au_data['onset'].values[0]\n",
    "            # gsr_data['Timestamp']=gsr_data['Timestamp']-gsr_data['Timestamp'].values[0]\n",
    "            au_data.head()\n",
    "            events = pd.read_csv(events_path, sep='\\t')\n",
    "            labels = pd.read_csv(labels_path, sep='\\t')\n",
    "            # Example of integrating events, this depends on how timestamps align\n",
    "            # This is a placeholder for merging or aligning logic\n",
    "            # Drop rows where 'onset' is NaN in both DataFrames\n",
    "            au_data = au_data.dropna(subset=['onset'])\n",
    "            events = events.dropna(subset=['onset'])\n",
    "\n",
    "            # Ensure both DataFrames are sorted by 'onset'\n",
    "            au_data = au_data.sort_values('onset')\n",
    "            events = events.sort_values('onset')\n",
    "\n",
    "            # Perform the merge_asof\n",
    "            au_data_merged = pd.merge_asof(au_data, events, on='onset', direction='backward')\n",
    "            au_list = ['onset','confidence',\n",
    " 'success','AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', \n",
    "                                                'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r', \n",
    "                                                'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r','duration', 'trial_type', 'flag', 'subject', 'run',\n",
    "                'trial', 'local_time', 'stim_file']\n",
    "            au_data=au_data_merged[au_list]\n",
    "            try:\n",
    "                \n",
    "                for stim_file in au_data['stim_file'].unique():\n",
    "                    metrics = {}\n",
    "                    # au_trial = extract_segment(au_data, stim_file, \"video\")\n",
    "                    au_trial = au_data[(au_data['stim_file'] == stim_file)]\n",
    "                    au_trial = au_trial.reset_index(drop=True)\n",
    "                    label_line=labels[labels['stim_file']==stim_file]\n",
    "                    metrics[\"AUs\"]=au_trial\n",
    "                    metrics['user']=user\n",
    "                    metrics['run']=run\n",
    "                    metrics['stim_file']=stim_file\n",
    "                    metrics['trial']=label_line['trial'].values[0]\n",
    "                    metrics['stim_emo']=label_line['trial_type'].values[0]\n",
    "                    metrics['preceived_arousal']=label_line['p_emotion_a'].values[0]\n",
    "                    metrics['preceived_valance']=label_line['p_emotion_v'].values[0]\n",
    "                    metrics['felt_arousal']=label_line['f_emotion_a'].values[0]\n",
    "                    metrics['felt_valance']=label_line['f_emotion_v'].values[0]\n",
    "                    metrics['openness'] = openness#+random.uniform(-2, 2)\n",
    "                    metrics['conscientiousness'] = conscientiousness#+random.uniform(-2, 2)\n",
    "                    metrics['extraversion'] = extraversion#+random.uniform(-2, 2)\n",
    "                    metrics['agreeableness'] = agreeableness#+random.uniform(-2, 2)\n",
    "                    metrics['neuroticism'] = neuroticism#+random.uniform(-2, 2)\n",
    "                    metrics_all.append(metrics)\n",
    "            except:\n",
    "                print(stim_file,user,run)\n",
    "                missing.append(stim_file+user+str(run))\n",
    "        else:\n",
    "            missing.append(stim_file+user+str(run))\n",
    "au_df=pd.DataFrame(metrics_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import random\n",
    "def extract_segment(dataframe, stim_file, start_flag,end_flag):\n",
    "    # Filter to get rows with matching 'stim_file' and 'flag'\n",
    "    filtered_df = dataframe[(dataframe['stim_file'] == stim_file)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        return None  # Return None if no matching rows found\n",
    "    \n",
    "    # Assuming the first matching row is the one to start from\n",
    "    initial_row = filtered_df.iloc[0]\n",
    "    initial_timestamp = filtered_df[filtered_df['flag']==start_flag]['onset'].iloc[0]\n",
    "    duration=0\n",
    "    start=0\n",
    "    for flag in filtered_df['flag'].unique():\n",
    "        if start==0 and flag!=start_flag:\n",
    "            start=1\n",
    "            continue\n",
    "        flag_filtered_df=filtered_df[filtered_df['flag']==flag]\n",
    "        initial_row = flag_filtered_df.iloc[0]\n",
    "        duration = duration+initial_row['duration']\n",
    "        if start==1 and flag==end_flag:\n",
    "            break\n",
    "    \n",
    "    # Calculate the end timestamp\n",
    "    end_timestamp = initial_timestamp + duration\n",
    "    \n",
    "    # Select rows from the initial timestamp up to and including where Timestamp <= end_timestamp\n",
    "    result_df = dataframe[(dataframe['onset'] >= initial_timestamp) & (dataframe['onset'] <= end_timestamp)]\n",
    "    \n",
    "    return result_df\n",
    "# Load participant data\n",
    "root_path=\"/Users/meis/Documents/Phd/fer_BIDS/\"\n",
    "# root_path=\"\"\n",
    "participants_df = pd.read_csv(root_path+\"participants.tsv\", sep=\"\\t\")\n",
    "gaze_json_path = '/Users/meis/Documents/codes/fer_BIDS/fer_BIDS/sub-acl/beh/sub-acl_task-fer_run-0_recording-gaze_physio.json'\n",
    "pupil_json_path = '/Users/meis/Documents/codes/fer_BIDS/fer_BIDS/sub-acl/beh/sub-acl_task-fer_run-0_recording-pupil_physio.json'\n",
    "with open(pupil_json_path, 'r') as file:\n",
    "    pupil_json_data = json.load(file)\n",
    "pupil_headers = pupil_json_data['Columns']\n",
    "with open(gaze_json_path, 'r') as file:\n",
    "    gaze_json_data = json.load(file)\n",
    "gaze_headers = gaze_json_data['Columns']\n",
    "missing = []\n",
    "metrics_all = []\n",
    "stim_file = \"\"\n",
    "headers=['onset', 'TIME', 'FPOGX', 'FPOGY', 'FPOGS', 'FPOGD', 'FPOGID', 'FPOGV',\n",
    "       'LPOGX', 'LPOGY', 'LPOGV', 'RPOGX', 'RPOGY', 'RPOGV', 'BPOGX', 'BPOGY',\n",
    "       'BPOGV', 'LPCX', 'LPCY', 'LPD', 'LPS', 'LPV', 'RPCX', 'RPCY', 'RPD',\n",
    "       'RPS', 'RPV', 'LEYEX', 'LEYEY', 'LEYEZ', 'LPUPILD', 'LPUPILV', 'REYEX',\n",
    "       'REYEY', 'REYEZ', 'RPUPILD', 'RPUPILV', 'duration', 'trial_type',\n",
    "       'flag', 'subject', 'run', 'trial', 'local_time', 'stim_file']\n",
    "# root_path=\"/Users/meis/Documents/Phd/fer_BIDS/\"\n",
    "for user in participants_df['participant_id'].unique():\n",
    "    user_df = participants_df[participants_df['participant_id'] == user]\n",
    "    openness = user_df['O'].values[0]\n",
    "    conscientiousness = user_df['C'].values[0]\n",
    "    extraversion = user_df['E'].values[0]\n",
    "    agreeableness = user_df['A'].values[0]\n",
    "    neuroticism = user_df['N'].values[0]\n",
    "    \n",
    "    for run in range(4):\n",
    "        print(user, run)\n",
    "        gaze_path = root_path+user + \"/beh/\" + user + \"_task-fer_run-\" + str(run) + \"_recording-gaze_physio.tsv.gz\"\n",
    "        pupil_path = root_path+user + \"/beh/\" + user + \"_task-fer_run-\" + str(run) + \"_recording-pupil_physio.tsv.gz\"\n",
    "        gaze_json_path = root_path+user + \"/beh/\" + user + \"_task-fer_run-\" + str(run) + \"_recording-gaze_physio.json\"\n",
    "        pupil_json_path = root_path+user + \"/beh/\" + user + \"_task-fer_run-\" + str(run) + \"_recording-pupil_physio.json\"\n",
    "        events_path =root_path+ user + \"/\" + user + \"_task-fer_run-\" + str(run) + \"_events.tsv\"\n",
    "        labels_path = root_path+user + \"/beh/\" + user + \"_task-fer_run-\" + str(run) + \"_beh.tsv\"\n",
    "        \n",
    "        if os.path.exists(gaze_json_path) and os.path.exists(pupil_json_path):\n",
    "                \n",
    "            # gaze_data = pd.read_csv(gaze_path, sep='\\t', compression='gzip',names=pupil_headers)\n",
    "            # pupil_data = pd.read_csv(pupil_path, sep='\\t', compression='gzip',names=gaze_headers)\n",
    "            gaze_data = pd.read_csv(gaze_path, sep='\\t', compression='gzip',names=pupil_headers)\n",
    "            pupil_data = pd.read_csv(pupil_path, sep='\\t', compression='gzip',names=gaze_headers).drop(columns=[\"TIME\"])\n",
    "            gaze_data['onset'] = gaze_data['onset'] - gaze_data['onset'].values[0]\n",
    "            pupil_data['onset'] = pupil_data['onset'] - pupil_data['onset'].values[0]\n",
    "            \n",
    "            gaze_data = gaze_data.dropna(subset=['onset'])\n",
    "            pupil_data = pupil_data.dropna(subset=['onset'])\n",
    "            \n",
    "            gaze_data = gaze_data.sort_values('onset')\n",
    "            pupil_data = pupil_data.sort_values('onset')\n",
    "            \n",
    "            eye_data_merged = pd.merge_asof(gaze_data, pupil_data, on=['onset'], direction='backward')\n",
    "\n",
    "            # eye_data_merged = pd.merge(gaze_data, pupil_data, on='onset', how='left')\n",
    "            events = pd.read_csv(events_path, sep='\\t').dropna(subset=['onset'])\n",
    "            labels = pd.read_csv(labels_path, sep='\\t')\n",
    "            \n",
    "            eye_data_merged = pd.merge_asof(eye_data_merged, events, on=['onset'], direction='backward')\n",
    "            eye_data_merged.columns=headers\n",
    "            for stim_file in eye_data_merged['stim_file'].unique():\n",
    "                try:\n",
    "                    metrics = {}\n",
    "                    # eye_trial = extract_segment(eye_data_merged, stim_file, 'second_fix','last_frame_video')\n",
    "                    eye_trial = eye_data_merged[(eye_data_merged['stim_file'] == stim_file)]\n",
    "                    eye_trial = eye_trial.reset_index(drop=True)\n",
    "                    label_line = labels[labels['stim_file'] == stim_file]\n",
    "                    # eyedata_headers=eye_trial.columns\n",
    "                    eye_trial.columns=headers\n",
    "                    metrics[\"Eye_Data\"] = eye_trial\n",
    "                    metrics['user'] = user\n",
    "                    metrics['run'] = run\n",
    "                    metrics['stim_file']=stim_file\n",
    "                    metrics['trial'] = label_line['trial'].values[0]\n",
    "                    metrics['stim_emo'] = label_line['trial_type'].values[0]\n",
    "                    metrics['preceived_arousal'] = label_line['p_emotion_a'].values[0]\n",
    "                    metrics['preceived_valance'] = label_line['p_emotion_v'].values[0]\n",
    "                    metrics['felt_arousal'] = label_line['f_emotion_a'].values[0]\n",
    "                    metrics['felt_valance'] = label_line['f_emotion_v'].values[0]\n",
    "                    metrics['openness'] = openness#+random.uniform(-2, 2)\n",
    "                    metrics['conscientiousness'] = conscientiousness#+random.uniform(-2, 2)\n",
    "                    metrics['extraversion'] = extraversion#+random.uniform(-2, 2)\n",
    "                    metrics['agreeableness'] = agreeableness#+random.uniform(-2, 2)\n",
    "                    metrics['neuroticism'] = neuroticism#+random.uniform(-2, 2)\n",
    "                    \n",
    "                    metrics_all.append(metrics)\n",
    "                except:\n",
    "                    print(stim_file, user, run)\n",
    "                    missing.append(stim_file + user + str(run))\n",
    "        else:\n",
    "            missing.append(stim_file + user + str(run))\n",
    "\n",
    "eye_df = pd.DataFrame(metrics_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import random\n",
    "participants_df=pd.read_csv(\"participants.tsv\",sep=\"\\t\")\n",
    "participants_df['participant_id'].unique()\n",
    "# Load headers from JSON\n",
    "# user='sub-agk'\n",
    "# run=0\n",
    "column_to_check = \"GSR_Conductance_cal\"\n",
    "def index_finder(data,start,end):\n",
    "    start_ind=0\n",
    "    end_ind=len(result)\n",
    "    for i,peak in enumerate(data['SCR_Onsets']):\n",
    "        # print(i,peak)\n",
    "        if peak<start:\n",
    "            start_ind=i\n",
    "        if peak<end:\n",
    "            end_ind=i\n",
    "    return start_ind,end_ind\n",
    "def extract_segment(dataframe, stim_file, flag):\n",
    "    # Filter to get rows with matching 'stim_file' and 'flag'\n",
    "    filtered_df = dataframe[(dataframe['stim_file'] == stim_file)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        return None  # Return None if no matching rows found\n",
    "    \n",
    "    # Assuming the first matching row is the one to start from\n",
    "    initial_row = filtered_df.iloc[0]\n",
    "    initial_timestamp = initial_row['onset']\n",
    "    duration=0\n",
    "    for flag in filtered_df['flag'].unique():\n",
    "        flag_filtered_df=filtered_df[filtered_df['flag']==flag]\n",
    "        initial_row = flag_filtered_df.iloc[0]\n",
    "        duration = duration+initial_row['duration']\n",
    "        if flag==\"trial\":\n",
    "            break\n",
    "    \n",
    "    # Calculate the end timestamp\n",
    "    end_timestamp = initial_timestamp + duration\n",
    "    \n",
    "    # Select rows from the initial timestamp up to and including where Timestamp <= end_timestamp\n",
    "    result_df = dataframe[(dataframe['onset'] >= initial_timestamp) & (dataframe['onset'] <= end_timestamp)]\n",
    "    \n",
    "    return result_df\n",
    "def generate_gsr_report(results,start_ind,end_ind):\n",
    "    \"\"\"\n",
    "    Generate a GSR report from the given results structure.\n",
    "\n",
    "    Parameters:\n",
    "    - results: dict, containing processed GSR metrics (e.g., SCR_Onsets, SCR_Peaks, etc.)\n",
    "\n",
    "    Returns:\n",
    "    - report: dict, a structured summary of the results.\n",
    "    \"\"\"\n",
    "    # Extract metrics\n",
    "    # scr_onsets = results['SCR_Onsets']\n",
    "    # scr_peaks = results['SCR_Peaks']\n",
    "    # scr_height = results['SCR_Height']\n",
    "    # scr_amplitude = results['SCR_Amplitude']\n",
    "    # scr_rise_time = results['SCR_RiseTime']\n",
    "    # scr_recovery = results['SCR_Recovery']\n",
    "    # scr_recovery_time = results['SCR_RecoveryTime']\n",
    "    # sampling_rate = results['sampling_rate']\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    scr_onsets = (results['SCR_Onsets']-start_ind)/(end_ind-start_ind)\n",
    "    scr_peaks = results['SCR_Peaks']\n",
    "    scr_height = results['SCR_Height']\n",
    "    scr_amplitude = results['SCR_Amplitude']\n",
    "    scr_rise_time = results['SCR_RiseTime']\n",
    "    scr_recovery = results['SCR_Recovery']\n",
    "    scr_recovery_time = results['SCR_RecoveryTime']\n",
    "    sampling_rate = results['sampling_rate']\n",
    "\n",
    "    # Calculate the number of peaks\n",
    "    num_peaks = len(scr_onsets)\n",
    "\n",
    "    # Prepare detailed metrics as a flattened dictionary\n",
    "    report = {\n",
    "        \"Number of Peaks\": num_peaks,\n",
    "\n",
    "        \"SCR_Onsets mean\": np.mean(scr_onsets) ,\n",
    "        \"SCR_Onsets median\": np.median(scr_onsets) ,\n",
    "        \"SCR_Onsets min\": np.min(scr_onsets) ,\n",
    "        \"SCR_Onsets max\": np.max(scr_onsets) ,\n",
    "        \"SCR_Onsets STD\": np.std(scr_onsets) ,\n",
    "        \n",
    "        \"SCR_Amplitude mean\": np.nanmean(scr_amplitude),\n",
    "        \"SCR_Amplitude median\": np.nanmedian(scr_amplitude),\n",
    "        \"SCR_Amplitude min\": np.nanmin(scr_amplitude),\n",
    "        \"SCR_Amplitude max\": np.nanmax(scr_amplitude),\n",
    "        \"SCR_Amplitude STD\": np.nanstd(scr_amplitude),\n",
    "        \n",
    "        \"SCR_Height mean\": np.nanmean(scr_height),\n",
    "        \"SCR_Height median\": np.nanmedian(scr_height),\n",
    "        \"SCR_Height min\": np.nanmin(scr_height),\n",
    "        \"SCR_Height max\": np.nanmax(scr_height),\n",
    "        \"SCR_Height STD\": np.nanstd(scr_height),\n",
    "        \n",
    "        \"SCR_RiseTime mean\": np.nanmean(scr_rise_time),\n",
    "        \"SCR_RiseTime median\": np.nanmedian(scr_rise_time),\n",
    "        \"SCR_RiseTime min\": np.nanmin(scr_rise_time),\n",
    "        \"SCR_RiseTime max\": np.nanmax(scr_rise_time),\n",
    "        \"SCR_RiseTime STD\": np.nanstd(scr_rise_time),\n",
    "        \n",
    "        \"SCR_Recovery mean\": np.nanmean(scr_recovery),\n",
    "        \"SCR_Recovery median\": np.nanmedian(scr_recovery),\n",
    "        \"SCR_Recovery min\": np.nanmin(scr_recovery),\n",
    "        \"SCR_Recovery max\": np.nanmax(scr_recovery),\n",
    "        \"SCR_Recovery STD\": np.nanstd(scr_recovery),\n",
    "        \n",
    "        \"SCR_RecoveryTime mean\": np.nanmean(scr_recovery_time),\n",
    "        \"SCR_RecoveryTime median\": np.nanmedian(scr_recovery_time),\n",
    "        \"SCR_RecoveryTime min\": np.nanmin(scr_recovery_time),\n",
    "        \"SCR_RecoveryTime max\": np.nanmax(scr_recovery_time),\n",
    "        \"SCR_RecoveryTime STD\": np.nanstd(scr_recovery_time),\n",
    "        \n",
    "        \"Sampling Rate\": sampling_rate\n",
    "    }\n",
    "\n",
    "    # Add detailed metrics to the report\n",
    "    # report = {\n",
    "    #     \"Detailed Metrics\": detailed_metrics\n",
    "    # }\n",
    "\n",
    "    return report\n",
    "def calculate_gsr_metrics_with_dynamic_range(data,flag, sampling_rate,start_delay=2,dwel_time=5):\n",
    "    \"\"\"\n",
    "    Calculate GSR metrics within a dynamic time range based on flags.\n",
    "    \n",
    "    Parameters:\n",
    "    - gsr_trial: dict or DataFrame, containing GSR data and flags.\n",
    "    - sampling_rate: int, the sampling rate of the data (e.g., 100 Hz).\n",
    "    \n",
    "    Returns:\n",
    "    - results: dict, containing GSR metrics in the specified format.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Ensure sampling_rate is an integer\n",
    "    samplerate = int(sampling_rate)\n",
    "    \n",
    "    # Extract flags and ensure they are a list\n",
    "    #flags = list(gsr_trial[\"flag\"]) if not isinstance(gsr_trial[\"flag\"], list) else gsr_trial[\"flag\"]\n",
    "    \n",
    "    # Extract raw GSR signal\n",
    "    # column_to_check = \"GSR_Conductance_cal\"  # Replace with the actual column name in your gsr_trial data\n",
    "    # rawGSRSignal = np.array(gsr_trial[column_to_check])\n",
    "    \n",
    "    # # Analyze GSR data using `eda.analyzeGSR`\n",
    "    # #data = eda.analyzeGSR(rawGSRSignal, samplerate, preprocessing=True, lowpass=1, highpass=0.05, phasic_seconds=4)\n",
    "    # peaks, data = nk.eda_peaks(nk.standardize(rawGSRSignal), sampling_rate=samplerate)\n",
    "    # Find start and end indices\n",
    "    try:\n",
    "        start_ind= int(start_delay*samplerate)+int(flag.index[flag == \"video\"][0])\n",
    "        end_ind= int(flag.index[flag == \"last_frame_video\"][0]) + int(dwel_time * samplerate)\n",
    "    except ValueError as e:\n",
    "        return {\"error\": f\"Could not find required flags: {str(e)}\"}\n",
    "    \n",
    "    # Convert data.keys() to a sorted list\n",
    "    #data_keys = sorted(data.keys())\n",
    "    # print(start_index,end_index)\n",
    "    # print(data)\n",
    "    start_index,end_index=index_finder(data,start_ind,end_ind)\n",
    "    # Adjust start and end indices\n",
    "    # print(start_index,end_index)\n",
    "    # start_index, end_index = max(0, start_index), min(len(data) - 1, end_index)\n",
    "    # print(start_index,end_index)\n",
    "    # data_cut=data[start_index:end_index]\n",
    "\n",
    "    sliced_data = {key: value[int(start_index):int(end_index) + 1] \n",
    "               for key, value in result.items() \n",
    "               if isinstance(value, np.ndarray)}\n",
    "    sliced_data['sampling_rate']=samplerate\n",
    "    # print(sliced_data)\n",
    "    # Determine start and end times\n",
    "    results=generate_gsr_report(sliced_data,start_ind,end_ind)\n",
    "    \n",
    "\n",
    "    return results\n",
    "missing=[]\n",
    "metrics_all=[]\n",
    "for user in participants_df['participant_id'].unique():\n",
    "    print(user)\n",
    "    user_df=participants_df[participants_df['participant_id']==user]\n",
    "    openness = user_df['O'].values[0]\n",
    "    conscientiousness = user_df['C'].values[0]\n",
    "    extraversion = user_df['E'].values[0]\n",
    "    agreeableness = user_df['A'].values[0]\n",
    "    neuroticism = user_df['N'].values[0]\n",
    "    for run in range(4):\n",
    "        \n",
    "        json_path=user+\"/beh/\"+user+\"_task-fer_run-\"+str(run)+\"_recording-gsr_physio.json\"\n",
    "        if os.path.exists(json_path):\n",
    "            gsr_path=user+\"/beh/\"+user+\"_task-fer_run-\"+str(run)+\"_recording-gsr_physio.tsv.gz\"\n",
    "            events_path=user+\"/\"+user+\"_task-fer_run-\"+str(run)+\"_events.tsv\"\n",
    "            labels_path=user+\"/beh/\"+user+\"_task-fer_run-\"+str(run)+\"_beh.tsv\"\n",
    "            with open(json_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            headers = json_data['Columns']\n",
    "            \n",
    "            # Read the gzipped TSV file using the loaded headers\n",
    "            # gsr_data= pd.read_csv(gsr_path, sep='\\t', compression='gzip')\n",
    "            gsr_data= pd.read_csv(gsr_path, sep='\\t', compression='gzip', names=headers)\n",
    "            gsr_data_raw=gsr_data.copy()\n",
    "            gsr_data['onset'] = pd.to_numeric(gsr_data['onset'], errors='coerce')\n",
    "            # gsr_data['Timestamp'] = pd.to_numeric(gsr_data['Timestamp'], errors='coerce')\n",
    "            # gsr_data['onset']=gsr_data['onset']-gsr_data['onset'].values[0]\n",
    "            # gsr_data['Timestamp']=gsr_data['Timestamp']-gsr_data['Timestamp'].values[0]\n",
    "            gsr_data.head()\n",
    "            events = pd.read_csv(events_path, sep='\\t')\n",
    "            labels = pd.read_csv(labels_path, sep='\\t')\n",
    "            # Example of integrating events, this depends on how timestamps align\n",
    "            # This is a placeholder for merging or aligning logic\n",
    "            # Drop rows where 'onset' is NaN in both DataFrames\n",
    "            gsr_data = gsr_data.dropna(subset=['onset'])\n",
    "            events = events.dropna(subset=['onset'])\n",
    "\n",
    "            # Ensure both DataFrames are sorted by 'onset'\n",
    "            gsr_data = gsr_data.sort_values('onset')\n",
    "            events = events.sort_values('onset')\n",
    "            # sampling_rate=len(gsr_data_merged['onset'])/(gsr_data_merged['onset'].iloc[0]-gsr_data_merged['onset'].iloc[-1])\n",
    "            # Perform the merge_asof\n",
    "            gsr_data_merged = pd.merge_asof(gsr_data, events, on='onset', direction='backward')\n",
    "            sampling_rate=len(gsr_data_merged['onset'])/(gsr_data_merged['onset'].iloc[-1]-gsr_data_merged['onset'].iloc[0])\n",
    "            \n",
    "            for stim_file in gsr_data_merged['stim_file'].unique():\n",
    "                try:\n",
    "                    gsr_trial = extract_segment(gsr_data_merged, stim_file, \"trial\")\n",
    "                    # gsr_trial=gsr_data_merged[gsr_data_merged['stim_file']==stim_file]\n",
    "                    gsr_trial = gsr_trial.reset_index(drop=True)\n",
    "                    \n",
    "                    sampling_rate = int(sampling_rate)  # Ensuring samplerate is an integer\n",
    "                    rawGSRSignal = np.array(gsr_trial[column_to_check])  # Ensuring the signal is in an acceptable format\n",
    "                    # print(sampling_rate)\n",
    "                    # Now call the function with corrected parameters\n",
    "                    data, result = nk.eda_process(nk.standardize(rawGSRSignal), sampling_rate=sampling_rate,method='neurokit')\n",
    "                    # print(data.columns)\n",
    "                    tonic=data[\"EDA_Tonic\"]\n",
    "                    phasic=data[\"EDA_Phasic\"]\n",
    "                    label_line=labels[labels['stim_file']==stim_file]\n",
    "                    # print(stim_file,user,run)\n",
    "                    metrics = calculate_gsr_metrics_with_dynamic_range(result,gsr_trial['flag'], sampling_rate,start_delay=0,dwel_time=10)\n",
    "                    metrics['user']=user\n",
    "                    metrics['run']=run\n",
    "                    metrics['stim_file']=stim_file\n",
    "                    metrics['trial'] = label_line['trial'].values[0]\n",
    "                    metrics['stim_emo']=label_line['trial_type'].values[0]\n",
    "                    metrics['preceived_arousal']=label_line['p_emotion_a'].values[0]\n",
    "                    metrics['preceived_valance']=label_line['p_emotion_v'].values[0]\n",
    "                    metrics['felt_arousal']=label_line['f_emotion_a'].values[0]\n",
    "                    metrics['felt_valance']=label_line['f_emotion_v'].values[0]\n",
    "                    metrics['openness'] = openness#+random.uniform(-2, 2)\n",
    "                    metrics['conscientiousness'] = conscientiousness#+random.uniform(-2, 2)\n",
    "                    metrics['extraversion'] = extraversion#+random.uniform(-2, 2)\n",
    "                    metrics['agreeableness'] = agreeableness#+random.uniform(-2, 2)\n",
    "                    metrics['neuroticism'] = neuroticism#+random.uniform(-2, 2)\n",
    "                    metrics_all.append(metrics)\n",
    "                except:\n",
    "                    print(stim_file,user,run)\n",
    "                    missing.append(stim_file+user+str(run))\n",
    "        else:\n",
    "            missing.append(stim_file+user+str(run))\n",
    "gsr_df=pd.DataFrame(metrics_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_df_pure=eye_df.drop(columns=['openness', 'conscientiousness', 'extraversion',\n",
    "       'agreeableness', 'neuroticism'])\n",
    "au_df_pure=au_df.drop(columns=['openness', 'conscientiousness', 'extraversion',\n",
    "       'agreeableness', 'neuroticism'])\n",
    "# gsr_df=df.drop(columns=['openness', 'conscientiousness', 'extraversion',\n",
    "#        'agreeableness', 'neuroticism'])\n",
    "\n",
    "df_merge=pd.merge(pd.merge(eye_df_pure,au_df_pure,on=['user', 'run', 'stim_file', 'trial', 'stim_emo','preceived_arousal', 'preceived_valance', 'felt_arousal','felt_valance'],  how='inner'),gsr_df,on=['user', 'run', 'stim_file', 'stim_emo','preceived_arousal', 'preceived_valance', 'felt_arousal','felt_valance'],  how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Overview\n",
    "\n",
    "This code is a comprehensive data processing pipeline that extracts and processes multimodal data (Action Units, Eye Tracking, and GSR) for a set of participants. The processing is modularized into helper functions and dedicated processing functions for each modality, ultimately merging the results into a single DataFrame for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Structure and Components\n",
    "\n",
    "### 1. Helper Functions\n",
    "\n",
    "- **get_personality_metrics**  \n",
    "  Extracts personality metrics (openness, conscientiousness, extraversion, agreeableness, neuroticism) from the participant data.\n",
    "\n",
    "- **merge_data_with_events**  \n",
    "  Merges any given data with corresponding event information using an as-of merge based on the 'onset' timestamp.\n",
    "\n",
    "- **extract_segment_single_flag & extract_segment_two_flags**  \n",
    "  Extracts segments from the data based on specified flag criteria:\n",
    "  - **Single Flag Version:** Extracts a segment starting from a given `stim_file`, summing durations until the flag `\"trial\"` is encountered.\n",
    "  - **Two Flags Version:** Extracts a segment using both a starting flag and an ending flag, summing durations accordingly.\n",
    "\n",
    "- **GSR-Related Helpers:**  \n",
    "  - **index_finder:** Identifies the start and end indices within the GSR data based on SCR onsets.\n",
    "  - **generate_gsr_report:** Calculates summary statistics (mean, median, min, max, STD) for various GSR metrics such as SCR Onsets, Amplitude, Height, Rise Time, Recovery, and Recovery Time.\n",
    "  - **calculate_gsr_metrics_with_dynamic_range:** Processes GSR data by dynamically determining the time window based on provided flags and generating a corresponding report.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Processing Functions\n",
    "\n",
    "- **process_au_data**  \n",
    "  Processes Action Unit (AU) data:\n",
    "  - Reads the necessary JSON and TSV files.\n",
    "  - Converts specified AU columns to numeric.\n",
    "  - Merges the AU data with event data.\n",
    "  - Extracts the relevant segment for each stimulus and compiles the results along with participant personality metrics.\n",
    "\n",
    "- **process_eye_data**  \n",
    "  Processes Eye Tracking data:\n",
    "  - Reads gaze and pupil data files along with their JSON metadata.\n",
    "  - Aligns and merges the two data sources.\n",
    "  - Merges the resulting eye data with event information.\n",
    "  - Extracts data for each stimulus and attaches corresponding personality metrics.\n",
    "\n",
    "- **process_gsr_data**  \n",
    "  Processes GSR data:\n",
    "  - Reads the GSR data and event files.\n",
    "  - Merges the GSR data with events.\n",
    "  - Uses NeuroKit2 to process the standardized GSR signal.\n",
    "  - Extracts the relevant segment using flag-based extraction and computes dynamic GSR metrics.\n",
    "  - Compiles these metrics with personality and trial-related information.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Main Function\n",
    "\n",
    "- **main**  \n",
    "  The central function that orchestrates the entire pipeline:\n",
    "  - Loads the participants file.\n",
    "  - Calls each of the modality-specific processing functions (AU, Eye, and GSR).\n",
    "  - Merges the processed data frames on common columns to create a unified dataset.\n",
    "  - Returns the merged DataFrame along with lists of any missing files or processing errors.\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Flow\n",
    "\n",
    "1. **Loading Data:**  \n",
    "   Participant data is loaded from a TSV file.\n",
    "\n",
    "2. **Processing Modalities:**  \n",
    "   - AU data, Eye Tracking data, and GSR data are processed independently using their respective functions.\n",
    "   - Each function reads the necessary files, performs data cleaning, merges with event data, and extracts segments relevant to each trial.\n",
    "\n",
    "3. **Merging Results:**  \n",
    "   The final step merges the pure data frames from AU and Eye processing with the GSR data frame based on shared identifiers (e.g., user, run, stim_file, trial information).\n",
    "\n",
    "4. **Error Handling:**  \n",
    "   Each processing function uses `try/except` blocks to handle missing or problematic files, logging any issues in a dedicated list for review.\n",
    "\n",
    "---\n",
    "\n",
    "## Customization and Adaptability\n",
    "\n",
    "- **Modality-Specific Settings:**  \n",
    "  Parameters such as the list of AU columns or GSR analysis thresholds can be modified to suit different datasets or experimental requirements.\n",
    "\n",
    "- **File Paths:**  \n",
    "  The code utilizes formatted strings for file paths, making it easy to adapt for dif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-agk 0\n",
      "sub-agk 1\n",
      "sub-agk 2\n",
      "sub-agk 3\n",
      "sub-bxn 0\n",
      "sub-bxn 1\n",
      "sub-bxn 2\n",
      "sub-bxn 3\n",
      "sub-ksu 0\n",
      "sub-ksu 1\n",
      "sub-ksu 2\n",
      "sub-ksu 3\n",
      "sub-bdn 0\n",
      "sub-bdn 1\n",
      "sub-bdn 2\n",
      "sub-bdn 3\n",
      "sub-tqu 0\n",
      "sub-tqu 1\n",
      "sub-tqu 2\n",
      "sub-tqu 3\n",
      "sub-bcz 0\n",
      "sub-bcz 1\n",
      "sub-bcz 2\n",
      "sub-bcz 3\n",
      "sub-ehk 0\n",
      "sub-ehk 1\n",
      "sub-ehk 2\n",
      "sub-ehk 3\n",
      "sub-mdt 0\n",
      "sub-mdt 1\n",
      "sub-mdt 2\n",
      "sub-mdt 3\n",
      "sub-lrc 0\n",
      "sub-lrc 1\n",
      "sub-lrc 2\n",
      "sub-lrc 3\n",
      "sub-cxy 0\n",
      "sub-cxy 1\n",
      "sub-cxy 2\n",
      "sub-cxy 3\n",
      "sub-ubc 0\n",
      "sub-ubc 1\n",
      "sub-ubc 2\n",
      "sub-ubc 3\n",
      "sub-nah 0\n",
      "sub-nah 1\n",
      "sub-nah 2\n",
      "sub-nah 3\n",
      "sub-dkf 0\n",
      "sub-dkf 1\n",
      "sub-dkf 2\n",
      "sub-dkf 3\n",
      "sub-xzc 0\n",
      "sub-xzc 1\n",
      "sub-xzc 2\n",
      "sub-xzc 3\n",
      "sub-fbj 0\n",
      "sub-fbj 1\n",
      "sub-fbj 2\n",
      "sub-fbj 3\n",
      "sub-yel 0\n",
      "sub-yel 1\n",
      "sub-yel 2\n",
      "sub-yel 3\n",
      "sub-tao 0\n",
      "sub-tao 1\n",
      "sub-tao 2\n",
      "sub-tao 3\n",
      "sub-tag 0\n",
      "sub-tag 1\n",
      "sub-tag 2\n",
      "sub-tag 3\n",
      "sub-ssn 0\n",
      "sub-ssn 1\n",
      "sub-ssn 2\n",
      "sub-ssn 3\n",
      "sub-hsc 0\n",
      "sub-hsc 1\n",
      "sub-hsc 2\n",
      "sub-hsc 3\n",
      "sub-acl 0\n",
      "sub-acl 1\n",
      "sub-acl 2\n",
      "sub-acl 3\n",
      "sub-ors 0\n",
      "sub-ors 1\n",
      "sub-ors 2\n",
      "sub-ors 3\n",
      "sub-rit 0\n",
      "sub-rit 1\n",
      "sub-rit 2\n",
      "sub-rit 3\n",
      "sub-zig 0\n",
      "sub-zig 1\n",
      "sub-zig 2\n",
      "sub-zig 3\n",
      "sub-oos 0\n",
      "sub-oos 1\n",
      "sub-oos 2\n",
      "sub-oos 3\n",
      "sub-jgs 0\n",
      "sub-jgs 1\n",
      "sub-jgs 2\n",
      "sub-jgs 3\n",
      "sub-zry 0\n",
      "sub-zry 1\n",
      "sub-zry 2\n",
      "sub-zry 3\n",
      "sub-pko 0\n",
      "sub-pko 1\n",
      "sub-pko 2\n",
      "sub-pko 3\n",
      "data/neu_its/1027_its_neu_xx.mp4 sub-pko 3 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-rhn 0\n",
      "sub-rhn 1\n",
      "sub-rhn 2\n",
      "sub-rhn 3\n",
      "sub-den 0\n",
      "sub-den 1\n",
      "data/neu_tie/1036_tie_neu_xx.mp4 sub-den 1 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-den 2\n",
      "sub-den 3\n",
      "sub-dip 0\n",
      "sub-dip 1\n",
      "sub-dip 2\n",
      "data/sad_its/1018_its_sad_xx.mp4 sub-dip 2 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-dip 3\n",
      "sub-adr 0\n",
      "sub-adr 1\n",
      "sub-adr 2\n",
      "sub-adr 3\n",
      "sub-pic 0\n",
      "sub-pic 1\n",
      "sub-pic 2\n",
      "sub-pic 3\n",
      "sub-mal 0\n",
      "sub-mal 1\n",
      "sub-mal 2\n",
      "sub-mal 3\n",
      "sub-jms 0\n",
      "sub-jms 1\n",
      "sub-jms 2\n",
      "sub-jms 3\n",
      "sub-pli 0\n",
      "sub-pli 1\n",
      "sub-pli 2\n",
      "sub-pli 3\n",
      "sub-eop 0\n",
      "sub-eop 1\n",
      "sub-eop 2\n",
      "sub-eop 3\n",
      "sub-fcd 0\n",
      "sub-fcd 1\n",
      "sub-fcd 2\n",
      "sub-fcd 3\n",
      "sub-etr 0\n",
      "sub-etr 1\n",
      "sub-etr 2\n",
      "sub-etr 3\n",
      "sub-mdl 0\n",
      "sub-mdl 1\n",
      "sub-mdl 2\n",
      "sub-mdl 3\n",
      "sub-k3d 0\n",
      "sub-k3d 1\n",
      "sub-k3d 2\n",
      "sub-k3d 3\n",
      "sub-m3p 0\n",
      "sub-m3p 1\n",
      "sub-m3p 2\n",
      "sub-m3p 3\n",
      "sub-xx2 0\n",
      "sub-xx2 1\n",
      "sub-xx2 2\n",
      "sub-xx2 3\n",
      "sub-qbf2 0\n",
      "sub-qbf2 1\n",
      "sub-qbf2 2\n",
      "sub-qbf2 3\n",
      "sub-cim4 0\n",
      "sub-cim4 1\n",
      "sub-cim4 2\n",
      "data/hap_ieo/1074_ieo_hap_hi.mp4 sub-cim4 2 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-cim4 3\n",
      "sub-m9g 0\n",
      "sub-m9g 1\n",
      "sub-m9g 2\n",
      "sub-m9g 3\n",
      "sub-ywh 0\n",
      "sub-ywh 1\n",
      "sub-ywh 2\n",
      "sub-ywh 3\n",
      "sub-aerj 0\n",
      "sub-aerj 1\n",
      "sub-aerj 2\n",
      "sub-aerj 3\n",
      "sub-bcxz 0\n",
      "sub-bcxz 1\n",
      "sub-bcxz 2\n",
      "sub-bcxz 3\n",
      "sub-cztf 0\n",
      "sub-cztf 1\n",
      "sub-cztf 2\n",
      "sub-cztf 3\n",
      "sub-yarq 0\n",
      "sub-yarq 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:172: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  au_data = pd.read_csv(au_path, sep='\\t', compression='gzip', names=headers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-yarq 2\n",
      "sub-yarq 3\n",
      "sub-ziym 0\n",
      "sub-ziym 1\n",
      "sub-ziym 2\n",
      "sub-ziym 3\n",
      "sub-kklo 0\n",
      "sub-kklo 1\n",
      "sub-kklo 2\n",
      "sub-kklo 3\n",
      "data/fea_ieo/1091_ieo_fea_md.mp4 sub-kklo 3 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-scuy 0\n",
      "sub-scuy 1\n",
      "sub-scuy 2\n",
      "sub-scuy 3\n",
      "sub-blgv 0\n",
      "sub-blgv 1\n",
      "sub-blgv 2\n",
      "sub-blgv 3\n",
      "sub-almn 0\n",
      "sub-almn 1\n",
      "sub-almn 2\n",
      "sub-almn 3\n",
      "sub-pkvd 0\n",
      "sub-pkvd 1\n",
      "sub-pkvd 2\n",
      "sub-pkvd 3\n",
      "sub-srfl 0\n",
      "sub-srfl 1\n",
      "sub-srfl 2\n",
      "sub-srfl 3\n",
      "sub-nvio 0\n",
      "sub-nvio 1\n",
      "sub-nvio 2\n",
      "sub-nvio 3\n",
      "sub-rokb 0\n",
      "sub-rokb 1\n",
      "sub-rokb 2\n",
      "sub-rokb 3\n",
      "sub-qwrt 0\n",
      "sub-qwrt 1\n",
      "sub-qwrt 2\n",
      "sub-qwrt 3\n",
      "sub-irma 0\n",
      "sub-irma 1\n",
      "sub-irma 2\n",
      "sub-irma 3\n",
      "sub-prvi 0\n",
      "sub-prvi 1\n",
      "sub-prvi 2\n",
      "sub-prvi 3\n",
      "sub-mlor 0\n",
      "sub-mlor 1\n",
      "sub-mlor 2\n",
      "sub-mlor 3\n",
      "sub-tico 0\n",
      "sub-tico 1\n",
      "sub-tico 2\n",
      "sub-tico 3\n",
      "sub-flrn 0\n",
      "sub-flrn 1\n",
      "sub-flrn 2\n",
      "sub-flrn 3\n",
      "sub-otsi 0\n",
      "sub-otsi 1\n",
      "sub-otsi 2\n",
      "sub-otsi 3\n",
      "sub-afri 0\n",
      "sub-afri 1\n",
      "sub-afri 2\n",
      "sub-afri 3\n",
      "sub-pwkb 0\n",
      "sub-pwkb 1\n",
      "sub-pwkb 2\n",
      "sub-pwkb 3\n",
      "sub-uwdm 0\n",
      "sub-uwdm 1\n",
      "sub-uwdm 2\n",
      "sub-uwdm 3\n",
      "sub-ymjj 0\n",
      "sub-ymjj 1\n",
      "sub-ymjj 2\n",
      "sub-ymjj 3\n",
      "sub-ptxm 0\n",
      "sub-ptxm 1\n",
      "sub-ptxm 2\n",
      "sub-ptxm 3\n",
      "sub-agk 0\n",
      "sub-agk 1\n",
      "sub-agk 2\n",
      "sub-agk 3\n",
      "sub-bxn 0\n",
      "sub-bxn 1\n",
      "sub-bxn 2\n",
      "sub-bxn 3\n",
      "sub-ksu 0\n",
      "sub-ksu 1\n",
      "sub-ksu 2\n",
      "sub-ksu 3\n",
      "sub-bdn 0\n",
      "sub-bdn 1\n",
      "sub-bdn 2\n",
      "sub-bdn 3\n",
      "sub-tqu 0\n",
      "sub-tqu 1\n",
      "sub-tqu 2\n",
      "sub-tqu 3\n",
      "sub-bcz 0\n",
      "sub-bcz 1\n",
      "sub-bcz 2\n",
      "sub-bcz 3\n",
      "sub-ehk 0\n",
      "sub-ehk 1\n",
      "sub-ehk 2\n",
      "sub-ehk 3\n",
      "sub-mdt 0\n",
      "sub-mdt 1\n",
      "sub-mdt 2\n",
      "sub-mdt 3\n",
      "sub-lrc 0\n",
      "sub-lrc 1\n",
      "sub-lrc 2\n",
      "sub-lrc 3\n",
      "sub-cxy 0\n",
      "sub-cxy 1\n",
      "sub-cxy 2\n",
      "sub-cxy 3\n",
      "sub-ubc 0\n",
      "sub-ubc 1\n",
      "sub-ubc 2\n",
      "sub-ubc 3\n",
      "sub-nah 0\n",
      "sub-nah 1\n",
      "sub-nah 2\n",
      "sub-nah 3\n",
      "sub-dkf 0\n",
      "sub-dkf 1\n",
      "sub-dkf 2\n",
      "sub-dkf 3\n",
      "sub-xzc 0\n",
      "sub-xzc 1\n",
      "sub-xzc 2\n",
      "sub-xzc 3\n",
      "sub-fbj 0\n",
      "sub-fbj 1\n",
      "sub-fbj 2\n",
      "sub-fbj 3\n",
      "sub-yel 0\n",
      "sub-yel 1\n",
      "sub-yel 2\n",
      "sub-yel 3\n",
      "sub-tao 0\n",
      "sub-tao 1\n",
      "sub-tao 2\n",
      "sub-tao 3\n",
      "sub-tag 0\n",
      "sub-tag 1\n",
      "sub-tag 2\n",
      "sub-tag 3\n",
      "sub-ssn 0\n",
      "sub-ssn 1\n",
      "sub-ssn 2\n",
      "sub-ssn 3\n",
      "sub-hsc 0\n",
      "sub-hsc 1\n",
      "sub-hsc 2\n",
      "sub-hsc 3\n",
      "sub-acl 0\n",
      "sub-acl 1\n",
      "sub-acl 2\n",
      "sub-acl 3\n",
      "sub-ors 0\n",
      "sub-ors 1\n",
      "sub-ors 2\n",
      "sub-ors 3\n",
      "sub-rit 0\n",
      "sub-rit 1\n",
      "sub-rit 2\n",
      "sub-rit 3\n",
      "sub-zig 0\n",
      "sub-zig 1\n",
      "sub-zig 2\n",
      "sub-zig 3\n",
      "sub-oos 0\n",
      "sub-oos 1\n",
      "sub-oos 2\n",
      "sub-oos 3\n",
      "sub-jgs 0\n",
      "sub-jgs 1\n",
      "sub-jgs 2\n",
      "sub-jgs 3\n",
      "sub-zry 0\n",
      "sub-zry 1\n",
      "sub-zry 2\n",
      "sub-zry 3\n",
      "sub-pko 0\n",
      "sub-pko 1\n",
      "sub-pko 2\n",
      "sub-pko 3\n",
      "data/neu_its/1027_its_neu_xx.mp4 sub-pko 3 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-rhn 0\n",
      "sub-rhn 1\n",
      "sub-rhn 2\n",
      "sub-rhn 3\n",
      "sub-den 0\n",
      "sub-den 1\n",
      "data/neu_tie/1036_tie_neu_xx.mp4 sub-den 1 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-den 2\n",
      "sub-den 3\n",
      "sub-dip 0\n",
      "sub-dip 1\n",
      "sub-dip 2\n",
      "data/sad_its/1018_its_sad_xx.mp4 sub-dip 2 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-dip 3\n",
      "sub-adr 0\n",
      "sub-adr 1\n",
      "sub-adr 2\n",
      "sub-adr 3\n",
      "sub-pic 0\n",
      "sub-pic 1\n",
      "sub-pic 2\n",
      "sub-pic 3\n",
      "sub-mal 0\n",
      "sub-mal 1\n",
      "sub-mal 2\n",
      "sub-mal 3\n",
      "sub-jms 0\n",
      "sub-jms 1\n",
      "sub-jms 2\n",
      "sub-jms 3\n",
      "sub-pli 0\n",
      "sub-pli 1\n",
      "sub-pli 2\n",
      "sub-pli 3\n",
      "sub-eop 0\n",
      "sub-eop 1\n",
      "sub-eop 2\n",
      "sub-eop 3\n",
      "sub-fcd 0\n",
      "sub-fcd 1\n",
      "sub-fcd 2\n",
      "sub-fcd 3\n",
      "sub-etr 0\n",
      "sub-etr 1\n",
      "sub-etr 2\n",
      "sub-etr 3\n",
      "sub-mdl 0\n",
      "sub-mdl 1\n",
      "sub-mdl 2\n",
      "sub-mdl 3\n",
      "sub-k3d 0\n",
      "sub-k3d 1\n",
      "sub-k3d 2\n",
      "sub-k3d 3\n",
      "sub-m3p 0\n",
      "sub-m3p 1\n",
      "sub-m3p 2\n",
      "sub-m3p 3\n",
      "sub-xx2 0\n",
      "sub-xx2 1\n",
      "sub-xx2 2\n",
      "sub-xx2 3\n",
      "sub-qbf2 0\n",
      "sub-qbf2 1\n",
      "sub-qbf2 2\n",
      "sub-qbf2 3\n",
      "sub-cim4 0\n",
      "sub-cim4 1\n",
      "sub-cim4 2\n",
      "data/hap_ieo/1074_ieo_hap_hi.mp4 sub-cim4 2 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-cim4 3\n",
      "sub-m9g 0\n",
      "sub-m9g 1\n",
      "sub-m9g 2\n",
      "sub-m9g 3\n",
      "sub-ywh 0\n",
      "sub-ywh 1\n",
      "sub-ywh 2\n",
      "sub-ywh 3\n",
      "sub-aerj 0\n",
      "sub-aerj 1\n",
      "sub-aerj 2\n",
      "sub-aerj 3\n",
      "sub-bcxz 0\n",
      "sub-bcxz 1\n",
      "sub-bcxz 2\n",
      "sub-bcxz 3\n",
      "sub-cztf 0\n",
      "sub-cztf 1\n",
      "sub-cztf 2\n",
      "sub-cztf 3\n",
      "sub-yarq 0\n",
      "sub-yarq 1\n",
      "sub-yarq 2\n",
      "sub-yarq 3\n",
      "sub-ziym 0\n",
      "sub-ziym 1\n",
      "sub-ziym 2\n",
      "sub-ziym 3\n",
      "sub-kklo 0\n",
      "sub-kklo 1\n",
      "sub-kklo 2\n",
      "sub-kklo 3\n",
      "data/fea_ieo/1091_ieo_fea_md.mp4 sub-kklo 3 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-scuy 0\n",
      "sub-scuy 1\n",
      "sub-scuy 2\n",
      "sub-scuy 3\n",
      "sub-blgv 0\n",
      "sub-blgv 1\n",
      "sub-blgv 2\n",
      "sub-blgv 3\n",
      "sub-almn 0\n",
      "sub-almn 1\n",
      "sub-almn 2\n",
      "sub-almn 3\n",
      "sub-pkvd 0\n",
      "sub-pkvd 1\n",
      "sub-pkvd 2\n",
      "sub-pkvd 3\n",
      "sub-srfl 0\n",
      "sub-srfl 1\n",
      "sub-srfl 2\n",
      "sub-srfl 3\n",
      "sub-nvio 0\n",
      "sub-nvio 1\n",
      "sub-nvio 2\n",
      "sub-nvio 3\n",
      "sub-rokb 0\n",
      "sub-rokb 1\n",
      "sub-rokb 2\n",
      "sub-rokb 3\n",
      "sub-qwrt 0\n",
      "sub-qwrt 1\n",
      "sub-qwrt 2\n",
      "sub-qwrt 3\n",
      "sub-irma 0\n",
      "sub-irma 1\n",
      "sub-irma 2\n",
      "sub-irma 3\n",
      "sub-prvi 0\n",
      "sub-prvi 1\n",
      "sub-prvi 2\n",
      "sub-prvi 3\n",
      "sub-mlor 0\n",
      "sub-mlor 1\n",
      "sub-mlor 2\n",
      "sub-mlor 3\n",
      "sub-tico 0\n",
      "sub-tico 1\n",
      "sub-tico 2\n",
      "sub-tico 3\n",
      "sub-flrn 0\n",
      "sub-flrn 1\n",
      "sub-flrn 2\n",
      "sub-flrn 3\n",
      "sub-otsi 0\n",
      "sub-otsi 1\n",
      "sub-otsi 2\n",
      "sub-otsi 3\n",
      "sub-afri 0\n",
      "sub-afri 1\n",
      "sub-afri 2\n",
      "sub-afri 3\n",
      "sub-pwkb 0\n",
      "sub-pwkb 1\n",
      "sub-pwkb 2\n",
      "sub-pwkb 3\n",
      "sub-uwdm 0\n",
      "sub-uwdm 1\n",
      "sub-uwdm 2\n",
      "sub-uwdm 3\n",
      "sub-ymjj 0\n",
      "sub-ymjj 1\n",
      "sub-ymjj 2\n",
      "sub-ymjj 3\n",
      "sub-ptxm 0\n",
      "sub-ptxm 1\n",
      "sub-ptxm 2\n",
      "sub-ptxm 3\n",
      "sub-agk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-bxn\n",
      "sub-ksu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-bdn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/neurokit2/eda/eda_peaks.py:127: RuntimeWarning: All-NaN slice encountered\n",
      "  info[\"SCR_Peaks\"] > np.nanmin(info[\"SCR_Onsets\"]), ~np.isnan(info[\"SCR_Onsets\"])\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:101: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Amplitude mean\": np.nanmean(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:102: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude median\": np.nanmedian(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:103: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude min\": np.nanmin(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:104: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude max\": np.nanmax(results['SCR_Amplitude']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:111: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RiseTime mean\": np.nanmean(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:112: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime median\": np.nanmedian(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:113: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime min\": np.nanmin(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:114: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime max\": np.nanmax(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-tqu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-bcz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-ehk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-mdt\n",
      "sub-lrc\n",
      "sub-cxy\n",
      "sub-ubc\n",
      "sub-nah\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/neurokit2/eda/eda_peaks.py:127: RuntimeWarning: All-NaN slice encountered\n",
      "  info[\"SCR_Peaks\"] > np.nanmin(info[\"SCR_Onsets\"]), ~np.isnan(info[\"SCR_Onsets\"])\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:101: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Amplitude mean\": np.nanmean(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:102: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude median\": np.nanmedian(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:103: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude min\": np.nanmin(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:104: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude max\": np.nanmax(results['SCR_Amplitude']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:111: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RiseTime mean\": np.nanmean(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:112: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime median\": np.nanmedian(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:113: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime min\": np.nanmin(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:114: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime max\": np.nanmax(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-dkf\n",
      "sub-xzc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-fbj\n",
      "sub-yel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-tao\n",
      "sub-tag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-ssn\n",
      "sub-hsc\n",
      "sub-acl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/neurokit2/eda/eda_peaks.py:127: RuntimeWarning: All-NaN slice encountered\n",
      "  info[\"SCR_Peaks\"] > np.nanmin(info[\"SCR_Onsets\"]), ~np.isnan(info[\"SCR_Onsets\"])\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:101: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Amplitude mean\": np.nanmean(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:102: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude median\": np.nanmedian(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:103: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude min\": np.nanmin(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:104: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude max\": np.nanmax(results['SCR_Amplitude']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:111: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RiseTime mean\": np.nanmean(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:112: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime median\": np.nanmedian(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:113: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime min\": np.nanmin(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:114: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime max\": np.nanmax(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-ors\n",
      "sub-rit\n",
      "sub-zig\n",
      "sub-oos\n",
      "sub-jgs\n",
      "sub-zry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-pko\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/neurokit2/eda/eda_peaks.py:127: RuntimeWarning: All-NaN slice encountered\n",
      "  info[\"SCR_Peaks\"] > np.nanmin(info[\"SCR_Onsets\"]), ~np.isnan(info[\"SCR_Onsets\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/neu_its/1027_its_neu_xx.mp4 sub-pko 3 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-rhn\n",
      "sub-den\n",
      "data/neu_tie/1036_tie_neu_xx.mp4 sub-den 1 The length of the input vector x must be greater than padlen, which is 15.\n",
      "sub-dip\n",
      "data/sad_its/1018_its_sad_xx.mp4 sub-dip 2 The length of the input vector x must be greater than padlen, which is 15.\n",
      "sub-adr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-pic\n",
      "sub-mal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-jms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-pli\n",
      "sub-eop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/neurokit2/eda/eda_peaks.py:127: RuntimeWarning: All-NaN slice encountered\n",
      "  info[\"SCR_Peaks\"] > np.nanmin(info[\"SCR_Onsets\"]), ~np.isnan(info[\"SCR_Onsets\"])\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:101: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Amplitude mean\": np.nanmean(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:102: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude median\": np.nanmedian(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:103: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude min\": np.nanmin(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:104: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude max\": np.nanmax(results['SCR_Amplitude']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:111: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RiseTime mean\": np.nanmean(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:112: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime median\": np.nanmedian(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:113: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime min\": np.nanmin(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:114: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime max\": np.nanmax(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-fcd\n",
      "sub-etr\n",
      "sub-mdl\n",
      "sub-k3d\n",
      "sub-m3p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-xx2\n",
      "sub-qbf2\n",
      "sub-cim4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/hap_ieo/1074_ieo_hap_hi.mp4 sub-cim4 2 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-m9g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-ywh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-aerj\n",
      "sub-bcxz\n",
      "sub-cztf\n",
      "sub-yarq\n",
      "sub-ziym\n",
      "sub-kklo\n",
      "data/fea_ieo/1091_ieo_fea_md.mp4 sub-kklo 3 index 0 is out of bounds for axis 0 with size 0\n",
      "sub-scuy\n",
      "sub-blgv\n",
      "sub-almn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-pkvd\n",
      "sub-srfl\n",
      "sub-nvio\n",
      "sub-rokb\n",
      "sub-qwrt\n",
      "sub-irma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/neurokit2/eda/eda_peaks.py:127: RuntimeWarning: All-NaN slice encountered\n",
      "  info[\"SCR_Peaks\"] > np.nanmin(info[\"SCR_Onsets\"]), ~np.isnan(info[\"SCR_Onsets\"])\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:101: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Amplitude mean\": np.nanmean(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:102: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude median\": np.nanmedian(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:103: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude min\": np.nanmin(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:104: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude max\": np.nanmax(results['SCR_Amplitude']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:111: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RiseTime mean\": np.nanmean(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:112: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime median\": np.nanmedian(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:113: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime min\": np.nanmin(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:114: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime max\": np.nanmax(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-prvi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/neurokit2/eda/eda_peaks.py:127: RuntimeWarning: All-NaN slice encountered\n",
      "  info[\"SCR_Peaks\"] > np.nanmin(info[\"SCR_Onsets\"]), ~np.isnan(info[\"SCR_Onsets\"])\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:101: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Amplitude mean\": np.nanmean(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:102: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude median\": np.nanmedian(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:103: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude min\": np.nanmin(results['SCR_Amplitude']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:104: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Amplitude max\": np.nanmax(results['SCR_Amplitude']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:111: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RiseTime mean\": np.nanmean(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:112: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime median\": np.nanmedian(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:113: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime min\": np.nanmin(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:114: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RiseTime max\": np.nanmax(results['SCR_RiseTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-mlor\n",
      "sub-tico\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-flrn\n",
      "sub-otsi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:116: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:117: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:118: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:119: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
      "/opt/anaconda3/envs/autogluon_env/lib/python3.9/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:121: RuntimeWarning: Mean of empty slice\n",
      "  \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:122: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:123: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
      "/var/folders/36/m_5br8615nn0xjj7xr694rq40000gn/T/ipykernel_88384/247883838.py:124: RuntimeWarning: All-NaN slice encountered\n",
      "  \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-afri\n",
      "sub-pwkb\n",
      "sub-uwdm\n",
      "sub-ymjj\n",
      "sub-ptxm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "\n",
    "# =============================================================================\n",
    "# Helper Functions\n",
    "# =============================================================================\n",
    "def get_personality_metrics(user_df):\n",
    "    \"\"\"Extract personality metrics from a participant's row.\"\"\"\n",
    "    return {\n",
    "        \"openness\": user_df[\"O\"].values[0],\n",
    "        \"conscientiousness\": user_df[\"C\"].values[0],\n",
    "        \"extraversion\": user_df[\"E\"].values[0],\n",
    "        \"agreeableness\": user_df[\"A\"].values[0],\n",
    "        \"neuroticism\": user_df[\"N\"].values[0]\n",
    "    }\n",
    "\n",
    "def merge_data_with_events(data, events_path, on='onset'):\n",
    "    \"\"\"Merge a data DataFrame with events using merge_asof.\"\"\"\n",
    "    events = pd.read_csv(events_path, sep='\\t').dropna(subset=[on])\n",
    "    data = data.dropna(subset=[on]).sort_values(on)\n",
    "    events = events.sort_values(on)\n",
    "    merged = pd.merge_asof(data, events, on=on, direction='backward')\n",
    "    return merged\n",
    "\n",
    "def extract_segment_single_flag(dataframe, stim_file, flag):\n",
    "    \"\"\"\n",
    "    Extract a segment based on a single flag.\n",
    "    \n",
    "    The function finds the first occurrence of stim_file and then sums durations\n",
    "    of subsequent unique flag groups until the flag \"trial\" is encountered.\n",
    "    \"\"\"\n",
    "    filtered_df = dataframe[dataframe['stim_file'] == stim_file]\n",
    "    if filtered_df.empty:\n",
    "        return None\n",
    "    initial_timestamp = filtered_df.iloc[0]['onset']\n",
    "    duration = 0\n",
    "    for f in filtered_df['flag'].unique():\n",
    "        flag_df = filtered_df[filtered_df['flag'] == f]\n",
    "        duration += flag_df.iloc[0]['duration']\n",
    "        if f == \"trial\":\n",
    "            break\n",
    "    end_timestamp = initial_timestamp + duration\n",
    "    result_df = dataframe[(dataframe['onset'] >= initial_timestamp) & \n",
    "                          (dataframe['onset'] <= end_timestamp)]\n",
    "    return result_df\n",
    "\n",
    "def extract_segment_two_flags(dataframe, stim_file, start_flag, end_flag):\n",
    "    \"\"\"\n",
    "    Extract a segment using a start and end flag.\n",
    "    \n",
    "    Starts at the first occurrence of start_flag and sums durations until the end_flag.\n",
    "    \"\"\"\n",
    "    filtered_df = dataframe[dataframe['stim_file'] == stim_file]\n",
    "    if filtered_df.empty:\n",
    "        return None\n",
    "    initial_timestamp = filtered_df[filtered_df['flag'] == start_flag]['onset'].iloc[0]\n",
    "    duration = 0\n",
    "    started = False\n",
    "    for f in filtered_df['flag'].unique():\n",
    "        if not started and f != start_flag:\n",
    "            started = True\n",
    "            continue\n",
    "        flag_df = filtered_df[filtered_df['flag'] == f]\n",
    "        duration += flag_df.iloc[0]['duration']\n",
    "        if started and f == end_flag:\n",
    "            break\n",
    "    end_timestamp = initial_timestamp + duration\n",
    "    result_df = dataframe[(dataframe['onset'] >= initial_timestamp) & \n",
    "                          (dataframe['onset'] <= end_timestamp)]\n",
    "    return result_df\n",
    "\n",
    "# --- GSR Helper Functions ---\n",
    "def index_finder(data, start, end):\n",
    "    \"\"\"Find start and end indices from the data's SCR_Onsets.\"\"\"\n",
    "    start_ind = 0\n",
    "    end_ind = len(data.get('SCR_Onsets', []))\n",
    "    for i, peak in enumerate(data.get('SCR_Onsets', [])):\n",
    "        if peak < start:\n",
    "            start_ind = i\n",
    "        if peak < end:\n",
    "            end_ind = i\n",
    "    return start_ind, end_ind\n",
    "\n",
    "def generate_gsr_report(results, start_ind, end_ind):\n",
    "    \"\"\"\n",
    "    Generate a GSR report from the given results structure.\n",
    "    Calculates summary statistics for each metric.\n",
    "    \"\"\"\n",
    "    scr_onsets = (results['SCR_Onsets'] - start_ind) / (end_ind - start_ind)\n",
    "    report = {\n",
    "        \"Number of Peaks\": len(scr_onsets),\n",
    "        \"SCR_Onsets mean\": np.mean(scr_onsets),\n",
    "        \"SCR_Onsets median\": np.median(scr_onsets),\n",
    "        \"SCR_Onsets min\": np.min(scr_onsets),\n",
    "        \"SCR_Onsets max\": np.max(scr_onsets),\n",
    "        \"SCR_Onsets STD\": np.std(scr_onsets),\n",
    "        \"SCR_Amplitude mean\": np.nanmean(results['SCR_Amplitude']),\n",
    "        \"SCR_Amplitude median\": np.nanmedian(results['SCR_Amplitude']),\n",
    "        \"SCR_Amplitude min\": np.nanmin(results['SCR_Amplitude']),\n",
    "        \"SCR_Amplitude max\": np.nanmax(results['SCR_Amplitude']),\n",
    "        \"SCR_Amplitude STD\": np.nanstd(results['SCR_Amplitude']),\n",
    "        \"SCR_Height mean\": np.nanmean(results['SCR_Height']),\n",
    "        \"SCR_Height median\": np.nanmedian(results['SCR_Height']),\n",
    "        \"SCR_Height min\": np.nanmin(results['SCR_Height']),\n",
    "        \"SCR_Height max\": np.nanmax(results['SCR_Height']),\n",
    "        \"SCR_Height STD\": np.nanstd(results['SCR_Height']),\n",
    "        \"SCR_RiseTime mean\": np.nanmean(results['SCR_RiseTime']),\n",
    "        \"SCR_RiseTime median\": np.nanmedian(results['SCR_RiseTime']),\n",
    "        \"SCR_RiseTime min\": np.nanmin(results['SCR_RiseTime']),\n",
    "        \"SCR_RiseTime max\": np.nanmax(results['SCR_RiseTime']),\n",
    "        \"SCR_RiseTime STD\": np.nanstd(results['SCR_RiseTime']),\n",
    "        \"SCR_Recovery mean\": np.nanmean(results['SCR_Recovery']),\n",
    "        \"SCR_Recovery median\": np.nanmedian(results['SCR_Recovery']),\n",
    "        \"SCR_Recovery min\": np.nanmin(results['SCR_Recovery']),\n",
    "        \"SCR_Recovery max\": np.nanmax(results['SCR_Recovery']),\n",
    "        \"SCR_Recovery STD\": np.nanstd(results['SCR_Recovery']),\n",
    "        \"SCR_RecoveryTime mean\": np.nanmean(results['SCR_RecoveryTime']),\n",
    "        \"SCR_RecoveryTime median\": np.nanmedian(results['SCR_RecoveryTime']),\n",
    "        \"SCR_RecoveryTime min\": np.nanmin(results['SCR_RecoveryTime']),\n",
    "        \"SCR_RecoveryTime max\": np.nanmax(results['SCR_RecoveryTime']),\n",
    "        \"SCR_RecoveryTime STD\": np.nanstd(results['SCR_RecoveryTime']),\n",
    "        \"Sampling Rate\": results['sampling_rate']\n",
    "    }\n",
    "    return report\n",
    "\n",
    "def calculate_gsr_metrics_with_dynamic_range(data, flags, sampling_rate, start_delay=2, dwel_time=5):\n",
    "    \"\"\"\n",
    "    Calculate GSR metrics within a dynamic time range based on flags.\n",
    "    \"\"\"\n",
    "    samplerate = int(sampling_rate)\n",
    "    try:\n",
    "        # These calculations depend on the flags index positions\n",
    "        start_ind_flag = int(start_delay * samplerate) + int(flags.index[flags == \"video\"][0])\n",
    "        end_ind_flag = int(flags.index[flags == \"last_frame_video\"][0]) + int(dwel_time * samplerate)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Could not find required flags: {str(e)}\"}\n",
    "    start_index, end_index = index_finder(data, start_ind_flag, end_ind_flag)\n",
    "    sliced_data = {key: value[int(start_index):int(end_index) + 1]\n",
    "                   for key, value in data.items() if isinstance(value, np.ndarray)}\n",
    "    sliced_data['sampling_rate'] = samplerate\n",
    "    results = generate_gsr_report(sliced_data, start_ind_flag, end_ind_flag)\n",
    "    return results\n",
    "\n",
    "# =============================================================================\n",
    "# Processing Functions for Each Modality\n",
    "# =============================================================================\n",
    "def process_au_data(participants_df, au_columns_subset):\n",
    "    \"\"\"Process Action Unit (AU) data for all participants and runs.\"\"\"\n",
    "    metrics_all = []\n",
    "    missing = []\n",
    "    for user in participants_df['participant_id'].unique():\n",
    "        user_df = participants_df[participants_df['participant_id'] == user]\n",
    "        personality = get_personality_metrics(user_df)\n",
    "        for run in range(4):\n",
    "            print(user, run)\n",
    "            json_path = f\"{user}/beh/{user}_task-fer_run-{run}_recording-videostream_physio.json\"\n",
    "            if not os.path.exists(json_path):\n",
    "                missing.append(f\"{user}_{run}_au\")\n",
    "                continue\n",
    "\n",
    "            au_path = f\"{user}/beh/{user}_task-fer_run-{run}_recording-videostream_physio.tsv.gz\"\n",
    "            events_path = f\"{user}/{user}_task-fer_run-{run}_events.tsv\"\n",
    "            labels_path = f\"{user}/beh/{user}_task-fer_run-{run}_beh.tsv\"\n",
    "\n",
    "            with open(json_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            headers = json_data['Columns']\n",
    "            au_data = pd.read_csv(au_path, sep='\\t', compression='gzip', names=headers)\n",
    "\n",
    "            # Convert selected columns to numeric\n",
    "            for tag in au_columns_subset:\n",
    "                au_data[tag] = pd.to_numeric(au_data[tag], errors='coerce')\n",
    "            au_data['onset'] = au_data['onset'] - au_data['onset'].values[0]\n",
    "            au_data = merge_data_with_events(au_data, events_path, on='onset')\n",
    "\n",
    "            # Keep only the desired columns\n",
    "            au_list = ['onset', 'confidence', 'success', 'AU01_r', 'AU02_r', 'AU04_r',\n",
    "                       'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r',\n",
    "                       'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r',\n",
    "                       'duration', 'trial_type', 'flag', 'subject', 'run',\n",
    "                       'trial', 'local_time', 'stim_file']\n",
    "            au_data = au_data[au_list]\n",
    "\n",
    "            try:\n",
    "                for stim_file in au_data['stim_file'].unique():\n",
    "                    metrics = {}\n",
    "                    # Here we use the entire segment for the given stim_file\n",
    "                    au_trial = au_data[au_data['stim_file'] == stim_file].reset_index(drop=True)\n",
    "                    label_line = pd.read_csv(labels_path, sep='\\t')\n",
    "                    label_line = label_line[label_line['stim_file'] == stim_file]\n",
    "\n",
    "                    metrics[\"AUs\"] = au_trial\n",
    "                    metrics.update({\n",
    "                        'user': user,\n",
    "                        'run': run,\n",
    "                        'stim_file': stim_file,\n",
    "                        'trial': label_line['trial'].values[0],\n",
    "                        'stim_emo': label_line['trial_type'].values[0],\n",
    "                        'preceived_arousal': label_line['p_emotion_a'].values[0],\n",
    "                        'preceived_valance': label_line['p_emotion_v'].values[0],\n",
    "                        'felt_arousal': label_line['f_emotion_a'].values[0],\n",
    "                        'felt_valance': label_line['f_emotion_v'].values[0],\n",
    "                    })\n",
    "                    metrics.update(personality)\n",
    "                    metrics_all.append(metrics)\n",
    "            except Exception as e:\n",
    "                print(stim_file, user, run, e)\n",
    "                missing.append(f\"{stim_file}_{user}_{run}_au\")\n",
    "    return pd.DataFrame(metrics_all), missing\n",
    "\n",
    "def process_eye_data(participants_df, root_path, headers):\n",
    "    \"\"\"Process Eye Tracking data for all participants and runs.\"\"\"\n",
    "    metrics_all = []\n",
    "    missing = []\n",
    "    for user in participants_df['participant_id'].unique():\n",
    "        user_df = participants_df[participants_df['participant_id'] == user]\n",
    "        personality = get_personality_metrics(user_df)\n",
    "        for run in range(4):\n",
    "            print(user, run)\n",
    "            gaze_path = os.path.join(root_path, f\"{user}/beh/{user}_task-fer_run-{run}_recording-gaze_physio.tsv.gz\")\n",
    "            pupil_path = os.path.join(root_path, f\"{user}/beh/{user}_task-fer_run-{run}_recording-pupil_physio.tsv.gz\")\n",
    "            gaze_json_path = os.path.join(root_path, f\"{user}/beh/{user}_task-fer_run-{run}_recording-gaze_physio.json\")\n",
    "            pupil_json_path = os.path.join(root_path, f\"{user}/beh/{user}_task-fer_run-{run}_recording-pupil_physio.json\")\n",
    "            events_path = os.path.join(root_path, f\"{user}/{user}_task-fer_run-{run}_events.tsv\")\n",
    "            labels_path = os.path.join(root_path, f\"{user}/beh/{user}_task-fer_run-{run}_beh.tsv\")\n",
    "\n",
    "            if not (os.path.exists(gaze_json_path) and os.path.exists(pupil_json_path)):\n",
    "                missing.append(f\"{user}_{run}_eye\")\n",
    "                continue\n",
    "\n",
    "            with open(pupil_json_path, 'r') as file:\n",
    "                pupil_json_data = json.load(file)\n",
    "            pupil_headers = pupil_json_data['Columns']\n",
    "            with open(gaze_json_path, 'r') as file:\n",
    "                gaze_json_data = json.load(file)\n",
    "            gaze_headers = gaze_json_data['Columns']\n",
    "\n",
    "            gaze_data = pd.read_csv(gaze_path, sep='\\t', compression='gzip', names=pupil_headers)\n",
    "            pupil_data = pd.read_csv(pupil_path, sep='\\t', compression='gzip', names=gaze_headers).drop(columns=[\"TIME\"])\n",
    "\n",
    "            gaze_data['onset'] = gaze_data['onset'] - gaze_data['onset'].values[0]\n",
    "            pupil_data['onset'] = pupil_data['onset'] - pupil_data['onset'].values[0]\n",
    "\n",
    "            gaze_data = gaze_data.dropna(subset=['onset']).sort_values('onset')\n",
    "            pupil_data = pupil_data.dropna(subset=['onset']).sort_values('onset')\n",
    "\n",
    "            eye_data_merged = pd.merge_asof(gaze_data, pupil_data, on='onset', direction='backward')\n",
    "            events = pd.read_csv(events_path, sep='\\t').dropna(subset=['onset'])\n",
    "            labels = pd.read_csv(labels_path, sep='\\t')\n",
    "\n",
    "            eye_data_merged = pd.merge_asof(eye_data_merged, events, on='onset', direction='backward')\n",
    "            eye_data_merged.columns = headers\n",
    "\n",
    "            for stim_file in eye_data_merged['stim_file'].unique():\n",
    "                try:\n",
    "                    metrics = {}\n",
    "                    eye_trial = eye_data_merged[eye_data_merged['stim_file'] == stim_file].reset_index(drop=True)\n",
    "                    label_line = labels[labels['stim_file'] == stim_file]\n",
    "                    eye_trial.columns = headers\n",
    "\n",
    "                    metrics[\"Eye_Data\"] = eye_trial\n",
    "                    metrics.update({\n",
    "                        'user': user,\n",
    "                        'run': run,\n",
    "                        'stim_file': stim_file,\n",
    "                        'trial': label_line['trial'].values[0],\n",
    "                        'stim_emo': label_line['trial_type'].values[0],\n",
    "                        'preceived_arousal': label_line['p_emotion_a'].values[0],\n",
    "                        'preceived_valance': label_line['p_emotion_v'].values[0],\n",
    "                        'felt_arousal': label_line['f_emotion_a'].values[0],\n",
    "                        'felt_valance': label_line['f_emotion_v'].values[0],\n",
    "                    })\n",
    "                    metrics.update(personality)\n",
    "                    metrics_all.append(metrics)\n",
    "                except Exception as e:\n",
    "                    print(stim_file, user, run, e)\n",
    "                    missing.append(f\"{stim_file}_{user}_{run}_eye\")\n",
    "    return pd.DataFrame(metrics_all), missing\n",
    "\n",
    "def process_gsr_data(participants_df, column_to_check):\n",
    "    \"\"\"Process GSR data for all participants and runs.\"\"\"\n",
    "    metrics_all = []\n",
    "    missing = []\n",
    "    for user in participants_df['participant_id'].unique():\n",
    "        print(user)\n",
    "        user_df = participants_df[participants_df['participant_id'] == user]\n",
    "        personality = get_personality_metrics(user_df)\n",
    "        for run in range(4):\n",
    "            json_path = f\"{user}/beh/{user}_task-fer_run-{run}_recording-gsr_physio.json\"\n",
    "            if not os.path.exists(json_path):\n",
    "                missing.append(f\"{user}_{run}_gsr\")\n",
    "                continue\n",
    "\n",
    "            gsr_path = f\"{user}/beh/{user}_task-fer_run-{run}_recording-gsr_physio.tsv.gz\"\n",
    "            events_path = f\"{user}/{user}_task-fer_run-{run}_events.tsv\"\n",
    "            labels_path = f\"{user}/beh/{user}_task-fer_run-{run}_beh.tsv\"\n",
    "\n",
    "            with open(json_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "            headers = json_data['Columns']\n",
    "            gsr_data = pd.read_csv(gsr_path, sep='\\t', compression='gzip', names=headers)\n",
    "            gsr_data['onset'] = pd.to_numeric(gsr_data['onset'], errors='coerce')\n",
    "            gsr_data = gsr_data.dropna(subset=['onset']).sort_values('onset')\n",
    "\n",
    "            events = pd.read_csv(events_path, sep='\\t').dropna(subset=['onset']).sort_values('onset')\n",
    "            gsr_data_merged = pd.merge_asof(gsr_data, events, on='onset', direction='backward')\n",
    "            sampling_rate = len(gsr_data_merged['onset']) / (\n",
    "                gsr_data_merged['onset'].iloc[-1] - gsr_data_merged['onset'].iloc[0]\n",
    "            )\n",
    "\n",
    "            for stim_file in gsr_data_merged['stim_file'].unique():\n",
    "                try:\n",
    "                    gsr_trial = extract_segment_single_flag(gsr_data_merged, stim_file, \"trial\")\n",
    "                    gsr_trial = gsr_trial.reset_index(drop=True)\n",
    "                    sampling_rate_int = int(sampling_rate)\n",
    "                    rawGSRSignal = np.array(gsr_trial[column_to_check])\n",
    "                    data, result = nk.eda_process(nk.standardize(rawGSRSignal),\n",
    "                                                  sampling_rate=sampling_rate_int,\n",
    "                                                  method='neurokit')\n",
    "                    # Optional: use tonic and phasic if needed\n",
    "                    tonic = data[\"EDA_Tonic\"]\n",
    "                    phasic = data[\"EDA_Phasic\"]\n",
    "\n",
    "                    label_line = pd.read_csv(labels_path, sep='\\t')\n",
    "                    label_line = label_line[label_line['stim_file'] == stim_file]\n",
    "\n",
    "                    metrics = calculate_gsr_metrics_with_dynamic_range(result, gsr_trial['flag'],\n",
    "                                                                       sampling_rate_int,\n",
    "                                                                       start_delay=0, dwel_time=10)\n",
    "                    metrics.update({\n",
    "                        'user': user,\n",
    "                        'run': run,\n",
    "                        'stim_file': stim_file,\n",
    "                        'trial': label_line['trial'].values[0],\n",
    "                        'stim_emo': label_line['trial_type'].values[0],\n",
    "                        'preceived_arousal': label_line['p_emotion_a'].values[0],\n",
    "                        'preceived_valance': label_line['p_emotion_v'].values[0],\n",
    "                        'felt_arousal': label_line['f_emotion_a'].values[0],\n",
    "                        'felt_valance': label_line['f_emotion_v'].values[0],\n",
    "                    })\n",
    "                    metrics.update(personality)\n",
    "                    metrics_all.append(metrics)\n",
    "                except Exception as e:\n",
    "                    print(stim_file, user, run, e)\n",
    "                    missing.append(f\"{stim_file}_{user}_{run}_gsr\")\n",
    "    return pd.DataFrame(metrics_all), missing\n",
    "\n",
    "# =============================================================================\n",
    "# Main Function\n",
    "# =============================================================================\n",
    "def main():\n",
    "    # Load participants file\n",
    "    participants_df = pd.read_csv(\"participants.tsv\", sep=\"\\t\")\n",
    "    \n",
    "    # --- Process AU Data ---\n",
    "    au_columns_subset = ['onset', 'confidence', 'success', 'AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r',\n",
    "                         'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r', 'AU17_r',\n",
    "                         'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r']\n",
    "    au_df, missing_au = process_au_data(participants_df, au_columns_subset)\n",
    "    \n",
    "    # --- Process Eye Data ---\n",
    "    eye_headers = ['onset', 'TIME', 'FPOGX', 'FPOGY', 'FPOGS', 'FPOGD', 'FPOGID', 'FPOGV',\n",
    "                   'LPOGX', 'LPOGY', 'LPOGV', 'RPOGX', 'RPOGY', 'RPOGV', 'BPOGX', 'BPOGY',\n",
    "                   'BPOGV', 'LPCX', 'LPCY', 'LPD', 'LPS', 'LPV', 'RPCX', 'RPCY', 'RPD',\n",
    "                   'RPS', 'RPV', 'LEYEX', 'LEYEY', 'LEYEZ', 'LPUPILD', 'LPUPILV', 'REYEX',\n",
    "                   'REYEY', 'REYEZ', 'RPUPILD', 'RPUPILV', 'duration', 'trial_type',\n",
    "                   'flag', 'subject', 'run', 'trial', 'local_time', 'stim_file']\n",
    "    root_path = \"/Users/meis/Documents/Phd/fer_BIDS/\"\n",
    "    eye_df, missing_eye = process_eye_data(participants_df, root_path, eye_headers)\n",
    "    \n",
    "    # --- Process GSR Data ---\n",
    "    column_to_check = \"GSR_Conductance_cal\"\n",
    "    gsr_df, missing_gsr = process_gsr_data(participants_df, column_to_check)\n",
    "    \n",
    "    # --- Merge DataFrames ---\n",
    "    # Drop personality columns from eye and AU for the merge\n",
    "    eye_df_pure = eye_df.drop(columns=['openness', 'conscientiousness', 'extraversion',\n",
    "                                         'agreeableness', 'neuroticism'])\n",
    "    au_df_pure = au_df.drop(columns=['openness', 'conscientiousness', 'extraversion',\n",
    "                                       'agreeableness', 'neuroticism'])\n",
    "    \n",
    "    merged_df = pd.merge(\n",
    "        pd.merge(eye_df_pure, au_df_pure,\n",
    "                 on=['user', 'run', 'stim_file', 'trial', 'stim_emo',\n",
    "                     'preceived_arousal', 'preceived_valance', 'felt_arousal', 'felt_valance'],\n",
    "                 how='inner'),\n",
    "        gsr_df,\n",
    "        on=['user', 'run', 'stim_file', 'stim_emo', 'preceived_arousal',\n",
    "            'preceived_valance', 'felt_arousal', 'felt_valance'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    return merged_df, missing_au, missing_eye, missing_gsr\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_df, missing_au, missing_eye, missing_gsr = main()\n",
    "    # You can now save or further process merged_df, and inspect missing_* lists if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Classification Pipeline\n",
    "\n",
    "This code implements a **multimodal emotion classification pipeline** that processes **GSR (Galvanic Skin Response), Eye Tracking, and Facial Action Unit (AU) data**. It extracts features, trains machine learning models using **AutoGluon**, evaluates models with **cross-validation**, and prints performance results in a **readable table format**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Pipeline Overview\n",
    "\n",
    "1. **Feature Extraction**  \n",
    "   - Summarizes **GSR, Eye Tracking, and AU** data into meaningful statistical features.\n",
    "   - Combines selected modalities based on user preferences.\n",
    "   - Extracts personality features with slight random variations.\n",
    "\n",
    "2. **Discretizing Emotion Labels**  \n",
    "   - Converts continuous emotion scores (**perceived arousal, perceived valence, felt arousal, felt valence**) into discrete classes using predefined thresholds.\n",
    "\n",
    "3. **Model Training with AutoGluon**  \n",
    "   - Trains **AutoGluon** models for each emotion category using tabular data.\n",
    "   - Splits data into training/testing and selects the best-performing model.\n",
    "   - Stores **F1 scores, accuracy, and model rankings** for evaluation.\n",
    "\n",
    "4. **Cross-Validation (5-Fold Stratified K-Fold)**  \n",
    "   - Performs **5-fold cross-validation** to assess model generalization.\n",
    "   - Computes **per-class F1 scores and accuracy metrics** for each emotion category.\n",
    "   - Re-trains the best model using **only the best-performing AutoGluon model** for each target.\n",
    "\n",
    "5. **Printing Results in a Readable Table**  \n",
    "   - Uses **tabulate** to format results in a clean and structured way.\n",
    "   - Displays **best models** along with **F1 scores and accuracy** in a grid format.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Key Functions**\n",
    "| Function | Description |\n",
    "|----------|------------|\n",
    "| `summarize_eye_data(eye_df)` | Computes mean, std, min, max for eye-tracking features. |\n",
    "| `summarize_au_data(au_df)` | Computes mean, std, min, max for AU features. |\n",
    "| `extract_features(eye_df, au_df, gsr_df, use_modalities)` | Merges modalities and extracts features. |\n",
    "| `discretize_labels(targets_df)` | Converts continuous emotion scores into discrete classes. |\n",
    "| `train_autogluon_models(features_df, targets_df)` | Trains AutoGluon models and selects best-performing models. |\n",
    "| `get_best_models(predictors)` | Retrieves the best non-ensemble models from AutoGluon. |\n",
    "| `retrain_best_model_kfold(features_df, targets_df, models, n_splits=5)` | Performs **K-fold cross-validation** using only the best models. |\n",
    "| `print_classification_results(cv_results, models, modalities)` | Formats and prints the classification results in a structured table. |\n",
    "\n",
    "---\n",
    "\n",
    "##  **Execution Steps**\n",
    "1. **Load data**: Prepare `eye_df`, `au_df`, and `gsr_df` from the dataset.\n",
    "2. **Extract features**:  \n",
    "   ```python\n",
    "   features_df, targets_df = extract_features(eye_df, au_df, gsr_df, use_modalities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import neurokit2 as nk\n",
    "\n",
    "###############################################################################\n",
    "# Step 1: Define Feature Extraction for Each Modality\n",
    "###############################################################################\n",
    "\n",
    "# Define the columns for each modality\n",
    "\n",
    "GSR_columns = [\n",
    "    'Number of Peaks', 'SCR_Onsets mean', 'SCR_Onsets median', 'SCR_Onsets min', 'SCR_Onsets max',\n",
    "    'SCR_Onsets STD', 'SCR_Amplitude mean', 'SCR_Amplitude median', 'SCR_Amplitude min', 'SCR_Amplitude max',\n",
    "    'SCR_Amplitude STD', 'SCR_Height mean', 'SCR_Height median', 'SCR_Height min', 'SCR_Height max',\n",
    "    'SCR_Height STD', 'SCR_RiseTime mean', 'SCR_RiseTime median', 'SCR_RiseTime min', 'SCR_RiseTime max',\n",
    "    'SCR_RiseTime STD', 'SCR_Recovery mean', 'SCR_Recovery median', 'SCR_Recovery min', 'SCR_Recovery max',\n",
    "    'SCR_Recovery STD', 'SCR_RecoveryTime mean', 'SCR_RecoveryTime median', 'SCR_RecoveryTime min',\n",
    "    'SCR_RecoveryTime max', 'SCR_RecoveryTime STD'\n",
    "]\n",
    "\n",
    "AU_columns = [\n",
    "    'AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU10_r', 'AU12_r',\n",
    "    'AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r', 'AU25_r', 'AU26_r', 'AU45_r'\n",
    "]\n",
    "\n",
    "Eye_columns = [\n",
    "    \"FPOGX\", \"FPOGY\", \"LPOGX\", \"LPOGY\", \"RPOGX\", \"RPOGY\",\n",
    "    \"FPOGD\", \"LPD\", \"RPD\", \"LPUPILD\", \"RPUPILD\"\n",
    "]\n",
    "\n",
    "Personality_columns = [\n",
    "    \"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"\n",
    "]\n",
    "\n",
    "def summarize_eye_data(eye_df):\n",
    "    \"\"\"\n",
    "    Summarize eye tracking data by computing mean, std, max, and min for each eye column.\n",
    "    \n",
    "    Args:\n",
    "        eye_df (DataFrame): DataFrame containing eye tracking data.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Summary statistics for each eye metric.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "    # Loop through each eye column defined in Eye_columns\n",
    "    for col in Eye_columns:\n",
    "        if col in eye_df.columns:\n",
    "            values = eye_df[col].dropna().values  # Remove missing values\n",
    "            # Compute summary statistics if data exists, else default to 0\n",
    "            summary[f\"{col}_mean\"] = np.mean(values) if len(values) > 0 else 0\n",
    "            summary[f\"{col}_std\"] = np.std(values) if len(values) > 0 else 0\n",
    "            summary[f\"{col}_max\"] = np.max(values) if len(values) > 0 else 0\n",
    "            summary[f\"{col}_min\"] = np.min(values) if len(values) > 0 else 0\n",
    "    return summary\n",
    "\n",
    "def summarize_au_data(au_df):\n",
    "    \"\"\"\n",
    "    Summarize action unit data by computing mean, std, max, and min for each AU column.\n",
    "    \n",
    "    Args:\n",
    "        au_df (DataFrame): DataFrame containing action unit data.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Summary statistics for each action unit.\n",
    "    \"\"\"\n",
    "    summary = {}\n",
    "    # Loop through each action unit column defined in AU_columns\n",
    "    for col in AU_columns:\n",
    "        if col in au_df.columns:\n",
    "            values = au_df[col].dropna().values  # Remove missing values\n",
    "            summary[f\"{col}_mean\"] = np.mean(values) if len(values) > 0 else 0\n",
    "            summary[f\"{col}_std\"] = np.std(values) if len(values) > 0 else 0\n",
    "            summary[f\"{col}_max\"] = np.max(values) if len(values) > 0 else 0\n",
    "            summary[f\"{col}_min\"] = np.min(values) if len(values) > 0 else 0\n",
    "    return summary\n",
    "\n",
    "def extract_features(eye_df, au_df, gsr_df, use_modalities):\n",
    "    \"\"\"\n",
    "    Extract and merge features from eye, action unit, and GSR data based on selected modalities.\n",
    "    \n",
    "    Args:\n",
    "        eye_df (DataFrame): Eye tracking data.\n",
    "        au_df (DataFrame): Action unit data.\n",
    "        gsr_df (DataFrame): GSR data (assumed pre-summarized).\n",
    "        use_modalities (dict): Dictionary specifying modalities to use.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame of combined features, DataFrame of target labels)\n",
    "    \"\"\"\n",
    "    # Merge data based on the specified modalities\n",
    "    if use_modalities.get(\"eye\", False) and use_modalities.get(\"action_units\", False):\n",
    "        df = pd.merge(\n",
    "            eye_df, au_df,\n",
    "            on=['user', 'run', 'stim_file', 'trial', 'stim_emo',\n",
    "                'preceived_arousal', 'preceived_valance', 'felt_arousal', 'felt_valance',\n",
    "                'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'],\n",
    "            how='inner'\n",
    "        )\n",
    "        if use_modalities.get(\"gsr\", False):\n",
    "            df = pd.merge(\n",
    "                df, gsr_df,\n",
    "                on=['user', 'run', 'stim_file', 'trial', 'stim_emo',\n",
    "                    'preceived_arousal', 'preceived_valance', 'felt_arousal', 'felt_valance',\n",
    "                    'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'],\n",
    "                how='inner'\n",
    "            )\n",
    "    elif use_modalities.get(\"eye\", False) and use_modalities.get(\"gsr\", False):\n",
    "        df = pd.merge(\n",
    "            eye_df, gsr_df,\n",
    "            on=['user', 'run', 'stim_file', 'trial', 'stim_emo',\n",
    "                'preceived_arousal', 'preceived_valance', 'felt_arousal', 'felt_valance',\n",
    "                'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'],\n",
    "            how='inner'\n",
    "        )\n",
    "    elif use_modalities.get(\"action_units\", False) and use_modalities.get(\"gsr\", False):\n",
    "        df = pd.merge(\n",
    "            au_df, gsr_df,\n",
    "            on=['user', 'run', 'stim_file', 'trial', 'stim_emo',\n",
    "                'preceived_arousal', 'preceived_valance', 'felt_arousal', 'felt_valance',\n",
    "                'openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'],\n",
    "            how='inner'\n",
    "        )\n",
    "    elif use_modalities.get(\"eye\", False):\n",
    "        df = eye_df\n",
    "    elif use_modalities.get(\"action_units\", False):\n",
    "        df = au_df\n",
    "    elif use_modalities.get(\"gsr\", False):\n",
    "        df = gsr_df\n",
    "\n",
    "    all_features, all_targets = [], []\n",
    "    \n",
    "    # Iterate over each row to compute feature summaries and extract target labels\n",
    "    for _, row in df.iterrows():\n",
    "        combined_features = {}\n",
    "\n",
    "        if use_modalities.get(\"eye\", False):\n",
    "            # Summarize eye data and update feature dictionary\n",
    "            eye_summary = summarize_eye_data(row[\"Eye_Data\"])\n",
    "            combined_features.update(eye_summary)\n",
    "\n",
    "        if use_modalities.get(\"action_units\", False):\n",
    "            # Summarize action unit data and update feature dictionary\n",
    "            au_summary = summarize_au_data(row[\"AUs\"])\n",
    "            combined_features.update(au_summary)\n",
    "\n",
    "        if use_modalities.get(\"gsr\", False):\n",
    "            # GSR data is assumed to be pre-summarized; update feature dictionary\n",
    "            gsr_summary = row[GSR_columns]\n",
    "            combined_features.update(gsr_summary)\n",
    "\n",
    "        if use_modalities.get(\"personality\", False):\n",
    "            # Add personality features with added random variation\n",
    "            personality_features = {\n",
    "                \"openness\": row[\"openness\"] + random.uniform(-2, 2),\n",
    "                \"conscientiousness\": row[\"conscientiousness\"] + random.uniform(-2, 2),\n",
    "                \"extraversion\": row[\"extraversion\"] + random.uniform(-2, 2),\n",
    "                \"agreeableness\": row[\"agreeableness\"] + random.uniform(-2, 2),\n",
    "                \"neuroticism\": row[\"neuroticism\"] + random.uniform(-2, 2)\n",
    "            }\n",
    "            combined_features.update(personality_features)\n",
    "\n",
    "        # Define target emotion labels\n",
    "        target_labels = {\n",
    "            \"preceived_arousal\": row[\"preceived_arousal\"],\n",
    "            \"preceived_valance\": row[\"preceived_valance\"],\n",
    "            \"felt_arousal\": row[\"felt_arousal\"],\n",
    "            \"felt_valance\": row[\"felt_valance\"]\n",
    "        }\n",
    "\n",
    "        all_features.append(combined_features)\n",
    "        all_targets.append(target_labels)\n",
    "\n",
    "    return pd.DataFrame(all_features), pd.DataFrame(all_targets)\n",
    "\n",
    "###############################################################################\n",
    "# Step 2: Discretize Emotional Labels (Optimal Binning)\n",
    "###############################################################################\n",
    "\n",
    "def discretize_labels(targets_df):\n",
    "    \"\"\"\n",
    "    Apply optimal binning to emotion labels.\n",
    "    \n",
    "    Args:\n",
    "        targets_df (DataFrame): DataFrame with continuous emotion labels.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with discretized emotion labels.\n",
    "    \"\"\"\n",
    "    bin_thresholds = {\n",
    "        \"preceived_arousal\": (4.8, 6.0),\n",
    "        \"preceived_valance\": (3.8, 5.5),\n",
    "        \"felt_arousal\": (4.6, 6.0),\n",
    "        \"felt_valance\": (4.3, 5.2)\n",
    "    }\n",
    "\n",
    "    # Discretize each target column based on threshold ranges\n",
    "    for col, (low_thr, high_thr) in bin_thresholds.items():\n",
    "        targets_df[col] = targets_df[col].apply(lambda x: 0 if x <= low_thr else (1 if x <= high_thr else 2))\n",
    "\n",
    "    return targets_df\n",
    "\n",
    "###############################################################################\n",
    "# Step 3: Train AutoGluon Models\n",
    "###############################################################################\n",
    "\n",
    "def train_autogluon_models(features_df, targets_df):\n",
    "    \"\"\"\n",
    "    Train AutoGluon models for each target emotion and compute performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        features_df (DataFrame): Input features.\n",
    "        targets_df (DataFrame): Target labels.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Dictionary of models, cross-validation results, leaderboards)\n",
    "    \"\"\"\n",
    "    models, cv_results, leaderboards = {}, {}, {}\n",
    "\n",
    "    # Iterate through each target column and train a model\n",
    "    for target_col in targets_df.columns:\n",
    "        print(f\"\\nTraining AutoGluon model for {target_col}...\")\n",
    "\n",
    "        y = targets_df[target_col]\n",
    "        # Split the data into training and testing sets with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features_df, y, test_size=0.2, stratify=y, random_state=123\n",
    "        )\n",
    "\n",
    "        # Combine training features with the target\n",
    "        df_combined_train = X_train.copy()\n",
    "        df_combined_train[target_col] = y_train\n",
    "\n",
    "        # Train the AutoGluon predictor with best quality presets, excluding slow KNN model\n",
    "        predictor = TabularPredictor(label=target_col, eval_metric=\"f1_macro\").fit(\n",
    "            df_combined_train,\n",
    "            presets=\"best_quality\",\n",
    "            excluded_model_types=['KNN'],\n",
    "            ag_args_fit={\"num_cpus\": os.cpu_count()}\n",
    "        )\n",
    "\n",
    "        # Retrieve the leaderboard and store the best model name\n",
    "        leaderboard = predictor.leaderboard(silent=True)\n",
    "        best_model_name = leaderboard.iloc[0][\"model\"]\n",
    "        leaderboards[target_col] = best_model_name\n",
    "\n",
    "        # Predict on the test set and compute F1 and accuracy metrics\n",
    "        y_pred = predictor.predict(X_test)\n",
    "        f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "        f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        cv_results[target_col] = {\n",
    "            \"F1 Low\": f\"{f1_per_class[0]:.4f}\",\n",
    "            \"F1 Medium\": f\"{f1_per_class[1]:.4f}\",\n",
    "            \"F1 High\": f\"{f1_per_class[2]:.4f}\",\n",
    "            \"F1 Macro\": f\"{f1_macro:.4f}\",\n",
    "            \"Accuracy\": f\"{accuracy:.4f}\"\n",
    "        }\n",
    "\n",
    "        models[target_col] = predictor\n",
    "\n",
    "    return models, cv_results, leaderboards\n",
    "\n",
    "###############################################################################\n",
    "# Step 4: Retrieve Best Models and Perform Cross-Validation\n",
    "###############################################################################\n",
    "\n",
    "MODEL_NAME_MAPPING = {\n",
    "    \"WeightedEnsemble_L2\": \"ENS_WEIGHTED\",\n",
    "    \"ExtraTreesEntr\": \"XT\",\n",
    "    \"ExtraTreesGini\": \"XT\",\n",
    "    \"LightGBM\": \"GBM\",\n",
    "    \"LightGBMXT\": \"GBM\",\n",
    "    \"NeuralNetTorch\": \"NN_TORCH\",\n",
    "    \"NeuralNetFastAI\": \"FASTAI\",\n",
    "    \"XGBoost\": \"XGB\",\n",
    "    \"CatBoost\": \"CAT\",\n",
    "    \"RandomForest\": \"RF\",\n",
    "    \"ExtraTrees\": \"XT\",\n",
    "    \"KNeighbors\": \"KNN\",\n",
    "    \"LogisticRegression\": \"LR\",\n",
    "    \"Transformer\": \"TRANSF\",\n",
    "    \"AG_TEXT_NN\": \"AG_TEXT_NN\",\n",
    "    \"AG_IMAGE_NN\": \"AG_IMAGE_NN\",\n",
    "    \"AG_AUTOMM\": \"AG_AUTOMM\",\n",
    "    \"FT_TRANSFORMER\": \"FT_TRANSFORMER\",\n",
    "    \"TABPFN\": \"TABPFN\",\n",
    "    \"TABPFNMIX\": \"TABPFNMIX\",\n",
    "    \"FASTTEXT\": \"FASTTEXT\",\n",
    "    \"ENS_WEIGHTED\": \"ENS_WEIGHTED\",\n",
    "    \"SIMPLE_ENS_WEIGHTED\": \"SIMPLE_ENS_WEIGHTED\",\n",
    "    \"IM_RULEFIT\": \"IM_RULEFIT\",\n",
    "    \"IM_GREEDYTREE\": \"IM_GREEDYTREE\",\n",
    "    \"IM_FIGS\": \"IM_FIGS\",\n",
    "    \"IM_HSTREE\": \"IM_HSTREE\",\n",
    "    \"IM_BOOSTEDRULES\": \"IM_BOOSTEDRULES\",\n",
    "    \"VW\": \"VW\",\n",
    "    \"DUMMY\": \"DUMMY\"\n",
    "}\n",
    "\n",
    "def get_best_models(predictors):\n",
    "    \"\"\"\n",
    "    Extract the best (non-ensemble) model for each emotion target from AutoGluon's leaderboard.\n",
    "    \n",
    "    Args:\n",
    "        predictors (dict): Dictionary of trained AutoGluon predictors.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Mapping from target column to best model name.\n",
    "    \"\"\"\n",
    "    best_models = {}\n",
    "    for target_col, predictor in predictors.items():\n",
    "        leaderboard = predictor.leaderboard(silent=True)\n",
    "        # Filter out ensemble models and select the best non-ensemble model\n",
    "        non_ensemble_models = leaderboard[~leaderboard[\"model\"].str.contains(\"WeightedEnsemble\")].copy()\n",
    "        best_model_name = non_ensemble_models.iloc[0][\"model\"]\n",
    "        best_models[target_col] = MODEL_NAME_MAPPING.get(best_model_name, best_model_name)\n",
    "    return best_models\n",
    "\n",
    "def retrain_best_model_kfold(features_df, targets_df, models, n_splits=5):\n",
    "    \"\"\"\n",
    "    Retrain the best AutoGluon models using K-Fold Cross-Validation.\n",
    "    \n",
    "    Args:\n",
    "        features_df (DataFrame): Input features.\n",
    "        targets_df (DataFrame): Target labels.\n",
    "        models (dict): Dictionary of trained models.\n",
    "        n_splits (int): Number of cross-validation splits.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Cross-validation results for each target.\n",
    "    \"\"\"\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "    cv_results = {}\n",
    "    best_models = get_best_models(models)\n",
    "    \n",
    "    for target_col, best_model_name in best_models.items():\n",
    "        print(f\"\\nPerforming {n_splits}-Fold Cross-Validation for {target_col} using {best_model_name}...\")\n",
    "        y = targets_df[target_col]\n",
    "        f1_scores_per_fold = {\"High\": [], \"Medium\": [], \"Low\": [], \"Macro Avg\": []}\n",
    "        accuracy_scores = []\n",
    "\n",
    "        # Loop through each fold in cross-validation\n",
    "        for train_idx, val_idx in kfold.split(features_df, y):\n",
    "            X_train_fold, X_val_fold = features_df.iloc[train_idx], features_df.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            # Combine training features with target\n",
    "            df_combined_train = X_train_fold.copy()\n",
    "            df_combined_train[target_col] = y_train_fold\n",
    "\n",
    "            # Train predictor using only the best model's hyperparameters\n",
    "            predictor = TabularPredictor(label=target_col).fit(\n",
    "                df_combined_train,\n",
    "                hyperparameters={best_model_name: {}},\n",
    "                keep_only_best=True,\n",
    "                num_bag_folds=0,  # Disable bagging\n",
    "                num_stack_levels=0,  # Disable stacking\n",
    "                presets=\"good_quality\",\n",
    "                ag_args_fit={\"num_cpus\": os.cpu_count()},\n",
    "                verbosity=0\n",
    "            )\n",
    "\n",
    "            # Predict on the validation fold and calculate performance metrics\n",
    "            y_val_preds = predictor.predict(X_val_fold)\n",
    "            f1_per_class = f1_score(y_val_fold, y_val_preds, average=None)\n",
    "            macro_f1 = f1_score(y_val_fold, y_val_preds, average=\"macro\")\n",
    "            accuracy = accuracy_score(y_val_fold, y_val_preds)\n",
    "\n",
    "            f1_scores_per_fold[\"High\"].append(f1_per_class[2])\n",
    "            f1_scores_per_fold[\"Medium\"].append(f1_per_class[1])\n",
    "            f1_scores_per_fold[\"Low\"].append(f1_per_class[0])\n",
    "            f1_scores_per_fold[\"Macro Avg\"].append(macro_f1)\n",
    "            accuracy_scores.append(accuracy)\n",
    "\n",
    "        # Store averaged CV metrics for this target\n",
    "        cv_results[target_col] = {\n",
    "            \"Model\": best_model_name,\n",
    "            \"High\": f\"{np.mean(f1_scores_per_fold['High']):.4f}  {np.std(f1_scores_per_fold['High']):.4f}\",\n",
    "            \"Medium\": f\"{np.mean(f1_scores_per_fold['Medium']):.4f}  {np.std(f1_scores_per_fold['Medium']):.4f}\",\n",
    "            \"Low\": f\"{np.mean(f1_scores_per_fold['Low']):.4f}  {np.std(f1_scores_per_fold['Low']):.4f}\",\n",
    "            \"Macro Avg\": f\"{np.mean(f1_scores_per_fold['Macro Avg']):.4f}  {np.std(f1_scores_per_fold['Macro Avg']):.4f}\",\n",
    "            \"Accuracy\": f\"{np.mean(accuracy_scores):.4f}  {np.std(accuracy_scores):.4f}\"\n",
    "        }\n",
    "    return cv_results\n",
    "\n",
    "def cross_validate_best_model(models, features_df, targets_df):\n",
    "    \"\"\"\n",
    "    Perform 5-fold cross-validation using the best AutoGluon models.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Dictionary of trained models.\n",
    "        features_df (DataFrame): Input features.\n",
    "        targets_df (DataFrame): Target labels.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Cross-validation results for each target.\n",
    "    \"\"\"\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    cv_results = {}\n",
    "\n",
    "    for target_col in targets_df.columns:\n",
    "        print(f\"\\nPerforming 5-Fold Cross-Validation for {target_col}...\")\n",
    "        y = targets_df[target_col]\n",
    "        f1_scores_per_fold = {\"High\": [], \"Medium\": [], \"Low\": [], \"Macro Avg\": []}\n",
    "        accuracy_scores = []\n",
    "\n",
    "        # Loop through each fold in cross-validation\n",
    "        for train_idx, val_idx in kfold.split(features_df, y):\n",
    "            X_train_fold, X_val_fold = features_df.iloc[train_idx], features_df.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            predictor = models[target_col]\n",
    "            predictor.refit_full()  # Retrain the model on full training data\n",
    "            y_val_preds = predictor.predict(X_val_fold)\n",
    "\n",
    "            f1_per_class = f1_score(y_val_fold, y_val_preds, average=None)\n",
    "            macro_f1 = f1_score(y_val_fold, y_val_preds, average=\"macro\")\n",
    "            accuracy = accuracy_score(y_val_fold, y_val_preds)\n",
    "\n",
    "            f1_scores_per_fold[\"High\"].append(f1_per_class[2])\n",
    "            f1_scores_per_fold[\"Medium\"].append(f1_per_class[1])\n",
    "            f1_scores_per_fold[\"Low\"].append(f1_per_class[0])\n",
    "            f1_scores_per_fold[\"Macro Avg\"].append(macro_f1)\n",
    "            accuracy_scores.append(accuracy)\n",
    "\n",
    "        # Save averaged CV metrics for current target\n",
    "        cv_results[target_col] = {\n",
    "            key: f\"{np.mean(scores):.4f}  {np.std(scores):.4f}\" for key, scores in f1_scores_per_fold.items()\n",
    "        }\n",
    "        cv_results[target_col][\"Accuracy\"] = f\"{np.mean(accuracy_scores):.4f}  {np.std(accuracy_scores):.4f}\"\n",
    "    return cv_results\n",
    "\n",
    "###############################################################################\n",
    "# Step 5: Generate LaTeX Table for Reporting Results\n",
    "###############################################################################\n",
    "\n",
    "def generate_latex_table(cv_results, models, modalities=\"Eye, Face, GSR, Personality\"):\n",
    "    \"\"\"\n",
    "    Generate a LaTeX table for classification results based on cross-validation metrics.\n",
    "    \n",
    "    Args:\n",
    "        cv_results (dict): Cross-validation results.\n",
    "        models (dict): Dictionary of trained models.\n",
    "        modalities (str): Description of the modalities used.\n",
    "        \n",
    "    Returns:\n",
    "        str: LaTeX formatted table as a string.\n",
    "    \"\"\"\n",
    "    # Retrieve best model names for each target by filtering out ensemble models\n",
    "    best_models = {}\n",
    "    for target_col, predictor in models.items():\n",
    "        leaderboard = predictor.leaderboard(silent=True)\n",
    "        non_ensemble_models = leaderboard[~leaderboard[\"model\"].str.contains(\"WeightedEnsemble\")].copy()\n",
    "        best_model_name = non_ensemble_models.iloc[0][\"model\"]\n",
    "        best_models[target_col] = best_model_name\n",
    "\n",
    "    # Start constructing the LaTeX table\n",
    "    latex_table = rf\"\"\"\n",
    "\\begin{{table*}}[h!]\n",
    "\\centering\n",
    "\\caption{{Classification Performance (F1 Score) Using Multimodal Features ({modalities}) (5-Fold Cross-Validation)}}\n",
    "\\label{{tab:multimodal_results}}\n",
    "\\begin{{tabular}}{{lcccc}}\n",
    "\\toprule\n",
    " & Perceived Arousal & Perceived Valence & Felt Arousal & Felt Valence \\\\ \n",
    "\\cmidrule(lr){{2-2}} \\cmidrule(lr){{3-3}} \\cmidrule(lr){{4-4}} \\cmidrule(lr){{5-5}}\n",
    "Best Model & {best_models[\"preceived_arousal\"]} & {best_models[\"preceived_valance\"]} & {best_models[\"felt_arousal\"]} & {best_models[\"felt_valance\"]} \\\\ \\midrule\n",
    "\"\"\"\n",
    "\n",
    "    # Add each performance metric row (High, Medium, Low, Macro Avg, Accuracy)\n",
    "    for class_label in [\"High\", \"Medium\", \"Low\", \"Macro Avg\", \"Accuracy\"]:\n",
    "        latex_table += (\n",
    "            f\"{class_label} & {cv_results['preceived_arousal'][class_label]} & \"\n",
    "            f\"{cv_results['preceived_valance'][class_label]} & \"\n",
    "            f\"{cv_results['felt_arousal'][class_label]} & \"\n",
    "            f\"{cv_results['felt_valance'][class_label]} \\\\\\\\ \\n\"\n",
    "        )\n",
    "\n",
    "    latex_table += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table*}\n",
    "\"\"\"\n",
    "    return latex_table\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import neurokit2 as nk\n",
    "from tabulate import tabulate  # Import tabulate for nice table formatting\n",
    "\n",
    "###############################################################################\n",
    "# Step 6: Print Classification Results in a Readable Table Format\n",
    "###############################################################################\n",
    "\n",
    "def print_classification_results(cv_results, models, modalities=\"Eye, Face, GSR, Personality\"):\n",
    "    \"\"\"\n",
    "    Print classification performance results in a formatted table.\n",
    "\n",
    "    Args:\n",
    "        cv_results (dict): Cross-validation results.\n",
    "        models (dict): Dictionary of trained models.\n",
    "        modalities (str): Description of the modalities used.\n",
    "    \"\"\"\n",
    "    # Retrieve best model names for each target\n",
    "    best_models = {}\n",
    "    for target_col, predictor in models.items():\n",
    "        leaderboard = predictor.leaderboard(silent=True)\n",
    "        non_ensemble_models = leaderboard[~leaderboard[\"model\"].str.contains(\"WeightedEnsemble\")].copy()\n",
    "        best_model_name = non_ensemble_models.iloc[0][\"model\"]\n",
    "        best_models[target_col] = best_model_name\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Classification Performance Using Multimodal Features ({modalities})\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Prepare the table header\n",
    "    headers = [\"Metric\", \"Perceived Arousal\", \"Perceived Valence\", \"Felt Arousal\", \"Felt Valence\"]\n",
    "\n",
    "    # Prepare rows for the best model names\n",
    "    best_model_row = [\"Best Model\", \n",
    "                      best_models.get(\"preceived_arousal\", \"N/A\"), \n",
    "                      best_models.get(\"preceived_valance\", \"N/A\"), \n",
    "                      best_models.get(\"felt_arousal\", \"N/A\"), \n",
    "                      best_models.get(\"felt_valance\", \"N/A\")]\n",
    "\n",
    "    # Prepare rows for performance metrics\n",
    "    metric_rows = []\n",
    "    for class_label in [\"High\", \"Medium\", \"Low\", \"Macro Avg\", \"Accuracy\"]:\n",
    "        metric_rows.append([\n",
    "            class_label,\n",
    "            cv_results[\"preceived_arousal\"].get(class_label, \"N/A\"),\n",
    "            cv_results[\"preceived_valance\"].get(class_label, \"N/A\"),\n",
    "            cv_results[\"felt_arousal\"].get(class_label, \"N/A\"),\n",
    "            cv_results[\"felt_valance\"].get(class_label, \"N/A\")\n",
    "        ])\n",
    "\n",
    "    # Print the formatted table\n",
    "    print(tabulate([best_model_row] + metric_rows, headers=headers, tablefmt=\"grid\"))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# Main Execution\n",
    "###############################################################################\n",
    "# Note: The main execution section should call the functions in the appropriate order.\n",
    "# For example:\n",
    "#   1. Load eye_df, au_df, and gsr_df from your data sources.\n",
    "#   2. Use extract_features() to merge and summarize the features.\n",
    "#   3. Discretize target labels with discretize_labels().\n",
    "#   4. Train models using train_autogluon_models().\n",
    "#   5. Optionally, perform cross-validation using retrain_best_model_kfold() or cross_validate_best_model().\n",
    "#   6. Generate a LaTeX table for reporting results with generate_latex_table().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Multimodal Emotion Classification\n",
    "\n",
    "## Overview\n",
    "This cell performs **emotion classification** using **all available modalities**:\n",
    "- **Eye Tracking**\n",
    "- **Facial Action Units (AU)**\n",
    "- **Galvanic Skin Response (GSR)**\n",
    "- **Personality Traits**\n",
    "\n",
    "## Steps:\n",
    "1. Extracts **features** from all modalities.\n",
    "2. **Discretizes emotion labels** (Perceived & Felt Arousal/Valence).\n",
    "3. Trains **AutoGluon models** for classification.\n",
    "4. Performs **5-Fold Cross-Validation** to validate performance.\n",
    "5. Prints results in a **structured table format**.\n",
    "\n",
    "## Expected Outcome:\n",
    "- The best-performing model for each emotion category.\n",
    "- **F1 scores & accuracy metrics** across different emotional states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250224_131604\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.21\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:22 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6041\n",
      "CPU Count:          14\n",
      "Memory Avail:       3.34 GB / 24.00 GB (13.9%)\n",
      "Disk Space Avail:   183.48 GB / 926.35 GB (19.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/meis/Documents/codes/fer_BIDS/fer_BIDS/AutogluonModels/ag-20250224_131604/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training AutoGluon model for preceived_arousal...\n"
     ]
    }
   ],
   "source": [
    "# Define modalities to be used\n",
    "USE_MODALITIES = {\n",
    "    \"eye\": True,\n",
    "    \"action_units\": True,\n",
    "    \"gsr\": True,\n",
    "    \"personality\": True\n",
    "}\n",
    "\n",
    "# Extract features and discretize target labels\n",
    "features_df, targets_df = extract_features(eye_df, au_df, gsr_df, USE_MODALITIES)\n",
    "targets_df = discretize_labels(targets_df)\n",
    "\n",
    "# Train AutoGluon models\n",
    "models_all, cv_results_all, leaderboard_all = train_autogluon_models(features_df, targets_df)\n",
    "\n",
    "# Perform 5-Fold Cross-Validation\n",
    "cv_results = retrain_best_model_kfold(features_df, targets_df, models_all, n_splits=5)\n",
    "\n",
    "# Print final classification results in a structured format\n",
    "print_classification_results(cv_results, models_all, modalities=\", \".join([k for k, v in USE_MODALITIES.items() if v]))\n",
    "\n",
    "# print(latex_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classification Using Eye Tracking Only\n",
    "\n",
    "## Overview\n",
    "This cell trains and evaluates models **using only Eye Tracking data**.\n",
    "\n",
    "## Steps:\n",
    "1. Extracts **features from eye-tracking data**.\n",
    "2. **Discretizes emotion labels** into categorical bins.\n",
    "3. Trains **AutoGluon models** based only on **eye-tracking features**.\n",
    "4. Performs **5-Fold Cross-Validation** for validation.\n",
    "5. Prints **classification results**.\n",
    "\n",
    "## Expected Outcome:\n",
    "- Performance analysis of **Eye Tracking** as a standalone modality.\n",
    "- Comparison with other modalities to assess its **effectiveness**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only eye-tracking data for classification\n",
    "USE_MODALITIES = {\n",
    "    \"eye\": True,\n",
    "    \"action_units\": False,\n",
    "    \"gsr\": False,\n",
    "    \"personality\": False\n",
    "}\n",
    "\n",
    "# Extract features and discretize target labels\n",
    "features_df, targets_df = extract_features(eye_df, au_df, gsr_df, USE_MODALITIES)\n",
    "targets_df = discretize_labels(targets_df)\n",
    "\n",
    "# Train models using only eye-tracking data\n",
    "models_eye, cv_results_eye, leaderboards_eye = train_autogluon_models(features_df, targets_df)\n",
    "\n",
    "# Perform 5-Fold Cross-Validation\n",
    "cv_results = retrain_best_model_kfold(features_df, targets_df, models_eye, n_splits=5)\n",
    "\n",
    "# Print classification results\n",
    "print_classification_results(cv_results, models_eye, modalities=\", \".join([k for k, v in USE_MODALITIES.items() if v]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classification Using Facial Action Units (AU) Only\n",
    "\n",
    "## Overview\n",
    "This cell evaluates **emotion classification performance using only AU data**.\n",
    "\n",
    "## Steps:\n",
    "1. Extracts **features from AU data**.\n",
    "2. **Discretizes emotion labels** into low, medium, and high bins.\n",
    "3. Trains **AutoGluon models** using **AU-based features**.\n",
    "4. Applies **5-Fold Cross-Validation**.\n",
    "5. Prints **classification results**.\n",
    "\n",
    "## Expected Outcome:\n",
    "- Measures how well **facial expressions alone** predict emotions.\n",
    "- Useful for **non-contact emotion recognition systems**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only action unit (facial expression) data for classification\n",
    "USE_MODALITIES = {\n",
    "    \"eye\": False,\n",
    "    \"action_units\": True,\n",
    "    \"gsr\": False,\n",
    "    \"personality\": False\n",
    "}\n",
    "\n",
    "# Extract features and discretize target labels\n",
    "features_df, targets_df = extract_features(eye_df, au_df, gsr_df, USE_MODALITIES)\n",
    "targets_df = discretize_labels(targets_df)\n",
    "\n",
    "# Train models using only facial action units\n",
    "models_au, cv_results_au, leaderboards_au = train_autogluon_models(features_df, targets_df)\n",
    "\n",
    "# Perform 5-Fold Cross-Validation\n",
    "cv_results = retrain_best_model_kfold(features_df, targets_df, models_au, n_splits=5)\n",
    "\n",
    "# Print classification results\n",
    "print_classification_results(cv_results, models_au, modalities=\", \".join([k for k, v in USE_MODALITIES.items() if v]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classification Using GSR Only\n",
    "\n",
    "## Overview\n",
    "This cell trains models **using only Galvanic Skin Response (GSR) data**.\n",
    "\n",
    "## Steps:\n",
    "1. Extracts **GSR-based features**.\n",
    "2. **Discretizes emotion labels**.\n",
    "3. Trains **AutoGluon models** using **only GSR signals**.\n",
    "4. Performs **5-Fold Cross-Validation**.\n",
    "5. Prints **classification results**.\n",
    "\n",
    "## Expected Outcome:\n",
    "- Evaluates the effectiveness of **physiological signals** for emotion detection.\n",
    "- Useful for **stress & arousal measurement applications**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only Galvanic Skin Response (GSR) data for classification\n",
    "USE_MODALITIES = {\n",
    "    \"eye\": False,\n",
    "    \"action_units\": False,\n",
    "    \"gsr\": True,\n",
    "    \"personality\": False\n",
    "}\n",
    "\n",
    "# Extract features and discretize target labels\n",
    "features_df, targets_df = extract_features(eye_df, au_df, gsr_df, USE_MODALITIES)\n",
    "targets_df = discretize_labels(targets_df)\n",
    "\n",
    "# Train models using only GSR data\n",
    "models_gsr, cv_results_gsr, leaderboards_gsr = train_autogluon_models(features_df, targets_df)\n",
    "\n",
    "# Perform 5-Fold Cross-Validation\n",
    "cv_results = retrain_best_model_kfold(features_df, targets_df, models_gsr, n_splits=5)\n",
    "\n",
    "# Print classification results\n",
    "print_classification_results(cv_results, models_gsr, modalities=\", \".join([k for k, v in USE_MODALITIES.items() if v]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Classification Using Eye, AU, and GSR (Excluding Personality)\n",
    "\n",
    "## Overview\n",
    "This cell performs **multimodal emotion classification** using:\n",
    "- **Eye Tracking**\n",
    "- **Facial Action Units (AU)**\n",
    "- **Galvanic Skin Response (GSR)**\n",
    "-  **Excludes Personality Traits**\n",
    "\n",
    "## Steps:\n",
    "1. Extracts **features** from Eye, AU, and GSR.\n",
    "2. **Discretizes emotion labels**.\n",
    "3. Trains **AutoGluon models** using the three selected modalities.\n",
    "4. Performs **5-Fold Cross-Validation**.\n",
    "5. Prints **classification results**.\n",
    "\n",
    "## Expected Outcome:\n",
    "- Evaluates multimodal performance **without personality features**.\n",
    "- Helps determine **if personality traits significantly impact classification**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Eye, AU, and GSR, but exclude personality features\n",
    "USE_MODALITIES = {\n",
    "    \"eye\": True,\n",
    "    \"action_units\": True,\n",
    "    \"gsr\": True,\n",
    "    \"personality\": False\n",
    "}\n",
    "\n",
    "# Extract features and discretize target labels\n",
    "features_df, targets_df = extract_features(eye_df, au_df, gsr_df, USE_MODALITIES)\n",
    "targets_df = discretize_labels(targets_df)\n",
    "\n",
    "# Train models with all modalities except personality\n",
    "models_all_without_personality, cv_results_no_personality, leaderboards_no_personality = train_autogluon_models(features_df, targets_df)\n",
    "\n",
    "# Perform 5-Fold Cross-Validation\n",
    "cv_results = retrain_best_model_kfold(features_df, targets_df, models_all_without_personality, n_splits=5)\n",
    "\n",
    "# Print classification results\n",
    "print_classification_results(cv_results, models_all_without_personality, modalities=\", \".join([k for k, v in USE_MODALITIES.items() if v]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Modalities Used in Each Cell\n",
    "\n",
    "| **Cell #** | **Modalities Used** | **Includes Personality?** | **Purpose** |\n",
    "|-----------|----------------|----------------|---------------------------|\n",
    "| **1** | Eye + AU + GSR + Personality |  Yes | Full Multimodal Analysis |\n",
    "| **2** | Eye Tracking Only |  No | Evaluates Eye Tracking Data Alone |\n",
    "| **3** | Facial Action Units Only |  No | Evaluates AU Data Alone |\n",
    "| **4** | GSR Only |  No | Evaluates GSR Data Alone |\n",
    "| **5** | Eye + AU + GSR |  No | Multimodal Without Personality |\n",
    "\n",
    "# Key Insights:\n",
    " **Modular Analysis**: Tests **individual vs. combined** modality performance.  \n",
    " **Robust Validation**: Uses **5-Fold Cross-Validation** to ensure reliable metrics.  \n",
    " **Interpretable Results**: Presents structured performance tables for comparison.  \n",
    " **Scalability**: Easy to modify and **extend** with new physiological data.\n",
    "\n",
    "# Next Steps:\n",
    "- **Compare results across cells** to identify the best modality.\n",
    "- **Optimize models** by tuning hyperparameters or using stacked ensembling.\n",
    "- **Expand the dataset** with EEG, HRV, or speech data for richer multimodal learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
