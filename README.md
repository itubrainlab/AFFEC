# AFFEC
Advancing Face-to-Face Emotion Communication: A Multimodal Dataset (AFFEC)
# Multimodal Emotion Classification

## ðŸ“Œ Overview
This project implements a **multimodal emotion classification pipeline** using **Eye Tracking, Facial Action Units (AU), Galvanic Skin Response (GSR), and Personality Traits**. The system extracts features, trains **AutoGluon-based machine learning models**, evaluates their performance with **5-Fold Cross-Validation**, and presents results in a **structured table format**.

This pipeline is designed for applications in **affective computing, human-computer interaction (HCI), and psychological research**.

---

## ðŸ“Š **Modality Breakdown**
This project allows flexible selection of **input modalities**:

| **Modality** | **Description** |
|-------------|----------------|
| **Eye Tracking** | Measures gaze, fixation, and pupil dilation. |
| **Facial Action Units (AU)** | Detects facial muscle movements and microexpressions. |
| **Galvanic Skin Response (GSR)** | Captures physiological arousal through skin conductance. |
| **Personality Traits** | Uses Big Five Personality Factors (OCEAN) for behavioral analysis. |

---

## ðŸš€ **How It Works**
### **1ï¸âƒ£ Feature Extraction**
- Extracts statistical features (**mean, std, min, max**) from **Eye Tracking, AU, and GSR** data.
- Introduces **randomized personality traits** for robustness.
- Merges selected modalities dynamically.

### **2ï¸âƒ£ Emotion Label Discretization**
- Converts **Perceived & Felt Arousal/Valence** into **Low, Medium, High** bins.

### **3ï¸âƒ£ Model Training**
- Uses **AutoGluon TabularPredictor** for **automated model selection & hyperparameter tuning**.
- Trains models for **each emotion category**.

### **4ï¸âƒ£ Results Evaluation**
- Performs **5-Fold Cross-Validation** to measure reliability.
- Displays **F1 Scores & Accuracy** in a structured table.

---

## ðŸ“Œ **Project Structure**
ðŸ“‚ multimodal-emotion-classification â”‚â”€â”€ ðŸ“„ README.md # Project documentation â”‚â”€â”€ ðŸ“„ requirements.txt # Dependencies â”‚â”€â”€ ðŸ“„ notebook.ipynb # Jupyter notebook containing code â”‚â”€â”€ ðŸ“‚ data # Folder to store input data (Eye, AU, GSR, Personality) â”‚â”€â”€ ðŸ“‚ models # Folder to save trained models â”‚â”€â”€ ðŸ“‚ results # Folder for storing classification results

---

## âš™ **Installation & Setup**
### **1ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
2ï¸âƒ£ Prepare Data (Download the data from [this link](https://doi.org/10.5281/zenodo.14794876))
Place your Eye Tracking, AU, GSR, and Personality data inside the /data directory.
Ensure the data follows the correct format.
3ï¸âƒ£ Run Jupyter Notebook
Launch Jupyter Notebook and open:
jupyter notebook notebook.ipynb
Run each cell to train models and evaluate results.

ðŸ›  Configuration
Modify the USE_MODALITIES dictionary in the notebook to enable/disable specific modalities:

python
Copy
Edit
USE_MODALITIES = {
    "eye": True,            # Use Eye Tracking data
    "action_units": True,   # Use Facial AU data
    "gsr": True,            # Use GSR data
    "personality": True     # Use Personality data
}
Set True to enable a modality.
Set False to disable a modality.
ðŸ“ˆ Example Output
diff
Copy
Edit
## Classification Performance Using Multimodal Features (Eye, Facial Action Units, GSR, Personality)

| Metric     | Perceived Arousal         | Perceived Valence         | Felt Arousal              | Felt Valence              |
|------------|---------------------------|---------------------------|---------------------------|---------------------------|
| Best Model | XGBoost                   | XGBoost                   | LightGBMXT                | NeuralNetFastAI           |
| High       | 0.4565 Â± 0.0160           | 0.2317 Â± 0.0249           | 0.2692 Â± 0.0401           | 0.4730 Â± 0.0304           |
| Medium     | 0.3619 Â± 0.0168           | 0.4306 Â± 0.0168           | 0.4845 Â± 0.0252           | 0.2938 Â± 0.0270           |
| Low        | 0.4945 Â± 0.0212           | 0.6104 Â± 0.0148           | 0.6797 Â± 0.0162           | 0.6133 Â± 0.0249           |
| Macro Avg  | 0.4377 Â± 0.0080           | 0.4242 Â± 0.0152           | 0.4778 Â± 0.0142           | 0.4600 Â± 0.0194           |
| Accuracy   | 0.4415 Â± 0.0085           | 0.5057 Â± 0.0145           | 0.5680 Â± 0.0158           | 0.5139 Â± 0.0218           |

ðŸ“Œ Experiment Configurations
The notebook allows different modality combinations to compare performance.

Configuration	Eye Tracking	AU	GSR	Personality	Purpose
Full Multimodal	âœ…	âœ…	âœ…	âœ…	Uses all available data.
Eye Tracking Only	âœ…	âŒ	âŒ	âŒ	Evaluates eye-tracking data in isolation.
Facial AU Only	âŒ	âœ…	âŒ	âŒ	Uses only facial expressions for classification.
GSR Only	âŒ	âŒ	âœ…	âŒ	Evaluates physiological arousal.
Multimodal (No Personality)	âœ…	âœ…	âœ…	âŒ	Tests classification without personality factors.
ðŸ”§ Requirements
All required dependencies are listed in requirements.txt.

Dependencies
bash
Copy
Edit
pip install -r requirements.txt
Software Requirements
Python >=3.8
Jupyter Notebook
AutoGluon
Pandas, NumPy, SciKit-Learn
NeuroKit2 (for GSR processing)
Tabulate (for formatted result display)
